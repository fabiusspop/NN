{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd090ec2-ba10-4996-ac66-8a979fd986b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbfb6bc-fdba-4d6d-b9b8-692ac1b5c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "T = [[0], [0], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfb9d20-b0bd-4ddf-b115-a9cf15c3d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLPClassifier(alpha = 1e-5, \n",
    "                    verbose = 1, \n",
    "                    max_iter = 3000, \n",
    "                    hidden_layer_sizes=(), \n",
    "                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4892d1-2854-4aaa-a859-89bf1bc3f8c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabius S Pop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1101: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.83083022\n",
      "Iteration 2, loss = 0.83022570\n",
      "Iteration 3, loss = 0.82962215\n",
      "Iteration 4, loss = 0.82901959\n",
      "Iteration 5, loss = 0.82841802\n",
      "Iteration 6, loss = 0.82781747\n",
      "Iteration 7, loss = 0.82721796\n",
      "Iteration 8, loss = 0.82661949\n",
      "Iteration 9, loss = 0.82602209\n",
      "Iteration 10, loss = 0.82542578\n",
      "Iteration 11, loss = 0.82483056\n",
      "Iteration 12, loss = 0.82423645\n",
      "Iteration 13, loss = 0.82364347\n",
      "Iteration 14, loss = 0.82305163\n",
      "Iteration 15, loss = 0.82246095\n",
      "Iteration 16, loss = 0.82187143\n",
      "Iteration 17, loss = 0.82128310\n",
      "Iteration 18, loss = 0.82069596\n",
      "Iteration 19, loss = 0.82011002\n",
      "Iteration 20, loss = 0.81952531\n",
      "Iteration 21, loss = 0.81894182\n",
      "Iteration 22, loss = 0.81835957\n",
      "Iteration 23, loss = 0.81777857\n",
      "Iteration 24, loss = 0.81719883\n",
      "Iteration 25, loss = 0.81662035\n",
      "Iteration 26, loss = 0.81604316\n",
      "Iteration 27, loss = 0.81546724\n",
      "Iteration 28, loss = 0.81489262\n",
      "Iteration 29, loss = 0.81431929\n",
      "Iteration 30, loss = 0.81374727\n",
      "Iteration 31, loss = 0.81317656\n",
      "Iteration 32, loss = 0.81260716\n",
      "Iteration 33, loss = 0.81203909\n",
      "Iteration 34, loss = 0.81147235\n",
      "Iteration 35, loss = 0.81090694\n",
      "Iteration 36, loss = 0.81034286\n",
      "Iteration 37, loss = 0.80978013\n",
      "Iteration 38, loss = 0.80921873\n",
      "Iteration 39, loss = 0.80865869\n",
      "Iteration 40, loss = 0.80809999\n",
      "Iteration 41, loss = 0.80754265\n",
      "Iteration 42, loss = 0.80698666\n",
      "Iteration 43, loss = 0.80643203\n",
      "Iteration 44, loss = 0.80587875\n",
      "Iteration 45, loss = 0.80532684\n",
      "Iteration 46, loss = 0.80477629\n",
      "Iteration 47, loss = 0.80422710\n",
      "Iteration 48, loss = 0.80367927\n",
      "Iteration 49, loss = 0.80313281\n",
      "Iteration 50, loss = 0.80258771\n",
      "Iteration 51, loss = 0.80204398\n",
      "Iteration 52, loss = 0.80150162\n",
      "Iteration 53, loss = 0.80096062\n",
      "Iteration 54, loss = 0.80042099\n",
      "Iteration 55, loss = 0.79988272\n",
      "Iteration 56, loss = 0.79934582\n",
      "Iteration 57, loss = 0.79881028\n",
      "Iteration 58, loss = 0.79827611\n",
      "Iteration 59, loss = 0.79774330\n",
      "Iteration 60, loss = 0.79721186\n",
      "Iteration 61, loss = 0.79668178\n",
      "Iteration 62, loss = 0.79615306\n",
      "Iteration 63, loss = 0.79562570\n",
      "Iteration 64, loss = 0.79509970\n",
      "Iteration 65, loss = 0.79457506\n",
      "Iteration 66, loss = 0.79405177\n",
      "Iteration 67, loss = 0.79352985\n",
      "Iteration 68, loss = 0.79300928\n",
      "Iteration 69, loss = 0.79249006\n",
      "Iteration 70, loss = 0.79197220\n",
      "Iteration 71, loss = 0.79145568\n",
      "Iteration 72, loss = 0.79094052\n",
      "Iteration 73, loss = 0.79042670\n",
      "Iteration 74, loss = 0.78991423\n",
      "Iteration 75, loss = 0.78940311\n",
      "Iteration 76, loss = 0.78889333\n",
      "Iteration 77, loss = 0.78838489\n",
      "Iteration 78, loss = 0.78787779\n",
      "Iteration 79, loss = 0.78737204\n",
      "Iteration 80, loss = 0.78686761\n",
      "Iteration 81, loss = 0.78636452\n",
      "Iteration 82, loss = 0.78586277\n",
      "Iteration 83, loss = 0.78536235\n",
      "Iteration 84, loss = 0.78486325\n",
      "Iteration 85, loss = 0.78436549\n",
      "Iteration 86, loss = 0.78386905\n",
      "Iteration 87, loss = 0.78337393\n",
      "Iteration 88, loss = 0.78288014\n",
      "Iteration 89, loss = 0.78238766\n",
      "Iteration 90, loss = 0.78189651\n",
      "Iteration 91, loss = 0.78140667\n",
      "Iteration 92, loss = 0.78091814\n",
      "Iteration 93, loss = 0.78043093\n",
      "Iteration 94, loss = 0.77994503\n",
      "Iteration 95, loss = 0.77946043\n",
      "Iteration 96, loss = 0.77897714\n",
      "Iteration 97, loss = 0.77849515\n",
      "Iteration 98, loss = 0.77801447\n",
      "Iteration 99, loss = 0.77753509\n",
      "Iteration 100, loss = 0.77705700\n",
      "Iteration 101, loss = 0.77658021\n",
      "Iteration 102, loss = 0.77610471\n",
      "Iteration 103, loss = 0.77563050\n",
      "Iteration 104, loss = 0.77515758\n",
      "Iteration 105, loss = 0.77468595\n",
      "Iteration 106, loss = 0.77421560\n",
      "Iteration 107, loss = 0.77374654\n",
      "Iteration 108, loss = 0.77327875\n",
      "Iteration 109, loss = 0.77281225\n",
      "Iteration 110, loss = 0.77234701\n",
      "Iteration 111, loss = 0.77188305\n",
      "Iteration 112, loss = 0.77142036\n",
      "Iteration 113, loss = 0.77095894\n",
      "Iteration 114, loss = 0.77049879\n",
      "Iteration 115, loss = 0.77003990\n",
      "Iteration 116, loss = 0.76958227\n",
      "Iteration 117, loss = 0.76912590\n",
      "Iteration 118, loss = 0.76867079\n",
      "Iteration 119, loss = 0.76821693\n",
      "Iteration 120, loss = 0.76776432\n",
      "Iteration 121, loss = 0.76731296\n",
      "Iteration 122, loss = 0.76686285\n",
      "Iteration 123, loss = 0.76641399\n",
      "Iteration 124, loss = 0.76596637\n",
      "Iteration 125, loss = 0.76551998\n",
      "Iteration 126, loss = 0.76507484\n",
      "Iteration 127, loss = 0.76463092\n",
      "Iteration 128, loss = 0.76418824\n",
      "Iteration 129, loss = 0.76374680\n",
      "Iteration 130, loss = 0.76330657\n",
      "Iteration 131, loss = 0.76286758\n",
      "Iteration 132, loss = 0.76242980\n",
      "Iteration 133, loss = 0.76199324\n",
      "Iteration 134, loss = 0.76155791\n",
      "Iteration 135, loss = 0.76112378\n",
      "Iteration 136, loss = 0.76069087\n",
      "Iteration 137, loss = 0.76025917\n",
      "Iteration 138, loss = 0.75982867\n",
      "Iteration 139, loss = 0.75939938\n",
      "Iteration 140, loss = 0.75897129\n",
      "Iteration 141, loss = 0.75854440\n",
      "Iteration 142, loss = 0.75811870\n",
      "Iteration 143, loss = 0.75769420\n",
      "Iteration 144, loss = 0.75727089\n",
      "Iteration 145, loss = 0.75684877\n",
      "Iteration 146, loss = 0.75642783\n",
      "Iteration 147, loss = 0.75600808\n",
      "Iteration 148, loss = 0.75558950\n",
      "Iteration 149, loss = 0.75517211\n",
      "Iteration 150, loss = 0.75475588\n",
      "Iteration 151, loss = 0.75434083\n",
      "Iteration 152, loss = 0.75392695\n",
      "Iteration 153, loss = 0.75351424\n",
      "Iteration 154, loss = 0.75310269\n",
      "Iteration 155, loss = 0.75269230\n",
      "Iteration 156, loss = 0.75228307\n",
      "Iteration 157, loss = 0.75187499\n",
      "Iteration 158, loss = 0.75146807\n",
      "Iteration 159, loss = 0.75106229\n",
      "Iteration 160, loss = 0.75065766\n",
      "Iteration 161, loss = 0.75025418\n",
      "Iteration 162, loss = 0.74985184\n",
      "Iteration 163, loss = 0.74945063\n",
      "Iteration 164, loss = 0.74905057\n",
      "Iteration 165, loss = 0.74865163\n",
      "Iteration 166, loss = 0.74825382\n",
      "Iteration 167, loss = 0.74785715\n",
      "Iteration 168, loss = 0.74746159\n",
      "Iteration 169, loss = 0.74706716\n",
      "Iteration 170, loss = 0.74667384\n",
      "Iteration 171, loss = 0.74628164\n",
      "Iteration 172, loss = 0.74589055\n",
      "Iteration 173, loss = 0.74550057\n",
      "Iteration 174, loss = 0.74511170\n",
      "Iteration 175, loss = 0.74472393\n",
      "Iteration 176, loss = 0.74433726\n",
      "Iteration 177, loss = 0.74395169\n",
      "Iteration 178, loss = 0.74356721\n",
      "Iteration 179, loss = 0.74318383\n",
      "Iteration 180, loss = 0.74280153\n",
      "Iteration 181, loss = 0.74242032\n",
      "Iteration 182, loss = 0.74204019\n",
      "Iteration 183, loss = 0.74166114\n",
      "Iteration 184, loss = 0.74128317\n",
      "Iteration 185, loss = 0.74090627\n",
      "Iteration 186, loss = 0.74053044\n",
      "Iteration 187, loss = 0.74015568\n",
      "Iteration 188, loss = 0.73978198\n",
      "Iteration 189, loss = 0.73940934\n",
      "Iteration 190, loss = 0.73903777\n",
      "Iteration 191, loss = 0.73866724\n",
      "Iteration 192, loss = 0.73829777\n",
      "Iteration 193, loss = 0.73792935\n",
      "Iteration 194, loss = 0.73756198\n",
      "Iteration 195, loss = 0.73719564\n",
      "Iteration 196, loss = 0.73683035\n",
      "Iteration 197, loss = 0.73646609\n",
      "Iteration 198, loss = 0.73610287\n",
      "Iteration 199, loss = 0.73574068\n",
      "Iteration 200, loss = 0.73537951\n",
      "Iteration 201, loss = 0.73501937\n",
      "Iteration 202, loss = 0.73466025\n",
      "Iteration 203, loss = 0.73430214\n",
      "Iteration 204, loss = 0.73394505\n",
      "Iteration 205, loss = 0.73358898\n",
      "Iteration 206, loss = 0.73323391\n",
      "Iteration 207, loss = 0.73287984\n",
      "Iteration 208, loss = 0.73252678\n",
      "Iteration 209, loss = 0.73217472\n",
      "Iteration 210, loss = 0.73182365\n",
      "Iteration 211, loss = 0.73147357\n",
      "Iteration 212, loss = 0.73112449\n",
      "Iteration 213, loss = 0.73077639\n",
      "Iteration 214, loss = 0.73042927\n",
      "Iteration 215, loss = 0.73008313\n",
      "Iteration 216, loss = 0.72973797\n",
      "Iteration 217, loss = 0.72939378\n",
      "Iteration 218, loss = 0.72905056\n",
      "Iteration 219, loss = 0.72870831\n",
      "Iteration 220, loss = 0.72836702\n",
      "Iteration 221, loss = 0.72802669\n",
      "Iteration 222, loss = 0.72768731\n",
      "Iteration 223, loss = 0.72734890\n",
      "Iteration 224, loss = 0.72701143\n",
      "Iteration 225, loss = 0.72667491\n",
      "Iteration 226, loss = 0.72633933\n",
      "Iteration 227, loss = 0.72600469\n",
      "Iteration 228, loss = 0.72567099\n",
      "Iteration 229, loss = 0.72533823\n",
      "Iteration 230, loss = 0.72500639\n",
      "Iteration 231, loss = 0.72467548\n",
      "Iteration 232, loss = 0.72434550\n",
      "Iteration 233, loss = 0.72401644\n",
      "Iteration 234, loss = 0.72368829\n",
      "Iteration 235, loss = 0.72336106\n",
      "Iteration 236, loss = 0.72303474\n",
      "Iteration 237, loss = 0.72270933\n",
      "Iteration 238, loss = 0.72238483\n",
      "Iteration 239, loss = 0.72206122\n",
      "Iteration 240, loss = 0.72173851\n",
      "Iteration 241, loss = 0.72141670\n",
      "Iteration 242, loss = 0.72109578\n",
      "Iteration 243, loss = 0.72077574\n",
      "Iteration 244, loss = 0.72045659\n",
      "Iteration 245, loss = 0.72013832\n",
      "Iteration 246, loss = 0.71982093\n",
      "Iteration 247, loss = 0.71950442\n",
      "Iteration 248, loss = 0.71918877\n",
      "Iteration 249, loss = 0.71887400\n",
      "Iteration 250, loss = 0.71856009\n",
      "Iteration 251, loss = 0.71824704\n",
      "Iteration 252, loss = 0.71793484\n",
      "Iteration 253, loss = 0.71762351\n",
      "Iteration 254, loss = 0.71731302\n",
      "Iteration 255, loss = 0.71700338\n",
      "Iteration 256, loss = 0.71669459\n",
      "Iteration 257, loss = 0.71638664\n",
      "Iteration 258, loss = 0.71607952\n",
      "Iteration 259, loss = 0.71577324\n",
      "Iteration 260, loss = 0.71546780\n",
      "Iteration 261, loss = 0.71516318\n",
      "Iteration 262, loss = 0.71485938\n",
      "Iteration 263, loss = 0.71455641\n",
      "Iteration 264, loss = 0.71425425\n",
      "Iteration 265, loss = 0.71395291\n",
      "Iteration 266, loss = 0.71365238\n",
      "Iteration 267, loss = 0.71335266\n",
      "Iteration 268, loss = 0.71305374\n",
      "Iteration 269, loss = 0.71275563\n",
      "Iteration 270, loss = 0.71245831\n",
      "Iteration 271, loss = 0.71216179\n",
      "Iteration 272, loss = 0.71186606\n",
      "Iteration 273, loss = 0.71157112\n",
      "Iteration 274, loss = 0.71127696\n",
      "Iteration 275, loss = 0.71098358\n",
      "Iteration 276, loss = 0.71069098\n",
      "Iteration 277, loss = 0.71039916\n",
      "Iteration 278, loss = 0.71010811\n",
      "Iteration 279, loss = 0.70981782\n",
      "Iteration 280, loss = 0.70952830\n",
      "Iteration 281, loss = 0.70923954\n",
      "Iteration 282, loss = 0.70895154\n",
      "Iteration 283, loss = 0.70866430\n",
      "Iteration 284, loss = 0.70837780\n",
      "Iteration 285, loss = 0.70809206\n",
      "Iteration 286, loss = 0.70780706\n",
      "Iteration 287, loss = 0.70752280\n",
      "Iteration 288, loss = 0.70723928\n",
      "Iteration 289, loss = 0.70695649\n",
      "Iteration 290, loss = 0.70667444\n",
      "Iteration 291, loss = 0.70639311\n",
      "Iteration 292, loss = 0.70611251\n",
      "Iteration 293, loss = 0.70583263\n",
      "Iteration 294, loss = 0.70555347\n",
      "Iteration 295, loss = 0.70527502\n",
      "Iteration 296, loss = 0.70499729\n",
      "Iteration 297, loss = 0.70472026\n",
      "Iteration 298, loss = 0.70444394\n",
      "Iteration 299, loss = 0.70416832\n",
      "Iteration 300, loss = 0.70389341\n",
      "Iteration 301, loss = 0.70361918\n",
      "Iteration 302, loss = 0.70334565\n",
      "Iteration 303, loss = 0.70307281\n",
      "Iteration 304, loss = 0.70280065\n",
      "Iteration 305, loss = 0.70252917\n",
      "Iteration 306, loss = 0.70225838\n",
      "Iteration 307, loss = 0.70198826\n",
      "Iteration 308, loss = 0.70171881\n",
      "Iteration 309, loss = 0.70145003\n",
      "Iteration 310, loss = 0.70118192\n",
      "Iteration 311, loss = 0.70091447\n",
      "Iteration 312, loss = 0.70064768\n",
      "Iteration 313, loss = 0.70038155\n",
      "Iteration 314, loss = 0.70011607\n",
      "Iteration 315, loss = 0.69985124\n",
      "Iteration 316, loss = 0.69958706\n",
      "Iteration 317, loss = 0.69932352\n",
      "Iteration 318, loss = 0.69906062\n",
      "Iteration 319, loss = 0.69879836\n",
      "Iteration 320, loss = 0.69853673\n",
      "Iteration 321, loss = 0.69827573\n",
      "Iteration 322, loss = 0.69801536\n",
      "Iteration 323, loss = 0.69775562\n",
      "Iteration 324, loss = 0.69749650\n",
      "Iteration 325, loss = 0.69723799\n",
      "Iteration 326, loss = 0.69698010\n",
      "Iteration 327, loss = 0.69672282\n",
      "Iteration 328, loss = 0.69646615\n",
      "Iteration 329, loss = 0.69621009\n",
      "Iteration 330, loss = 0.69595463\n",
      "Iteration 331, loss = 0.69569976\n",
      "Iteration 332, loss = 0.69544550\n",
      "Iteration 333, loss = 0.69519182\n",
      "Iteration 334, loss = 0.69493874\n",
      "Iteration 335, loss = 0.69468624\n",
      "Iteration 336, loss = 0.69443433\n",
      "Iteration 337, loss = 0.69418299\n",
      "Iteration 338, loss = 0.69393224\n",
      "Iteration 339, loss = 0.69368206\n",
      "Iteration 340, loss = 0.69343245\n",
      "Iteration 341, loss = 0.69318341\n",
      "Iteration 342, loss = 0.69293493\n",
      "Iteration 343, loss = 0.69268702\n",
      "Iteration 344, loss = 0.69243966\n",
      "Iteration 345, loss = 0.69219286\n",
      "Iteration 346, loss = 0.69194662\n",
      "Iteration 347, loss = 0.69170092\n",
      "Iteration 348, loss = 0.69145577\n",
      "Iteration 349, loss = 0.69121117\n",
      "Iteration 350, loss = 0.69096710\n",
      "Iteration 351, loss = 0.69072357\n",
      "Iteration 352, loss = 0.69048058\n",
      "Iteration 353, loss = 0.69023812\n",
      "Iteration 354, loss = 0.68999619\n",
      "Iteration 355, loss = 0.68975479\n",
      "Iteration 356, loss = 0.68951391\n",
      "Iteration 357, loss = 0.68927355\n",
      "Iteration 358, loss = 0.68903370\n",
      "Iteration 359, loss = 0.68879437\n",
      "Iteration 360, loss = 0.68855555\n",
      "Iteration 361, loss = 0.68831724\n",
      "Iteration 362, loss = 0.68807944\n",
      "Iteration 363, loss = 0.68784214\n",
      "Iteration 364, loss = 0.68760534\n",
      "Iteration 365, loss = 0.68736903\n",
      "Iteration 366, loss = 0.68713322\n",
      "Iteration 367, loss = 0.68689790\n",
      "Iteration 368, loss = 0.68666307\n",
      "Iteration 369, loss = 0.68642872\n",
      "Iteration 370, loss = 0.68619486\n",
      "Iteration 371, loss = 0.68596147\n",
      "Iteration 372, loss = 0.68572856\n",
      "Iteration 373, loss = 0.68549613\n",
      "Iteration 374, loss = 0.68526417\n",
      "Iteration 375, loss = 0.68503267\n",
      "Iteration 376, loss = 0.68480164\n",
      "Iteration 377, loss = 0.68457108\n",
      "Iteration 378, loss = 0.68434097\n",
      "Iteration 379, loss = 0.68411133\n",
      "Iteration 380, loss = 0.68388213\n",
      "Iteration 381, loss = 0.68365339\n",
      "Iteration 382, loss = 0.68342510\n",
      "Iteration 383, loss = 0.68319725\n",
      "Iteration 384, loss = 0.68296985\n",
      "Iteration 385, loss = 0.68274289\n",
      "Iteration 386, loss = 0.68251636\n",
      "Iteration 387, loss = 0.68229028\n",
      "Iteration 388, loss = 0.68206462\n",
      "Iteration 389, loss = 0.68183940\n",
      "Iteration 390, loss = 0.68161460\n",
      "Iteration 391, loss = 0.68139023\n",
      "Iteration 392, loss = 0.68116628\n",
      "Iteration 393, loss = 0.68094275\n",
      "Iteration 394, loss = 0.68071964\n",
      "Iteration 395, loss = 0.68049694\n",
      "Iteration 396, loss = 0.68027465\n",
      "Iteration 397, loss = 0.68005277\n",
      "Iteration 398, loss = 0.67983130\n",
      "Iteration 399, loss = 0.67961023\n",
      "Iteration 400, loss = 0.67938957\n",
      "Iteration 401, loss = 0.67916930\n",
      "Iteration 402, loss = 0.67894943\n",
      "Iteration 403, loss = 0.67872996\n",
      "Iteration 404, loss = 0.67851087\n",
      "Iteration 405, loss = 0.67829218\n",
      "Iteration 406, loss = 0.67807387\n",
      "Iteration 407, loss = 0.67785595\n",
      "Iteration 408, loss = 0.67763840\n",
      "Iteration 409, loss = 0.67742124\n",
      "Iteration 410, loss = 0.67720445\n",
      "Iteration 411, loss = 0.67698804\n",
      "Iteration 412, loss = 0.67677199\n",
      "Iteration 413, loss = 0.67655632\n",
      "Iteration 414, loss = 0.67634102\n",
      "Iteration 415, loss = 0.67612607\n",
      "Iteration 416, loss = 0.67591149\n",
      "Iteration 417, loss = 0.67569727\n",
      "Iteration 418, loss = 0.67548341\n",
      "Iteration 419, loss = 0.67526991\n",
      "Iteration 420, loss = 0.67505675\n",
      "Iteration 421, loss = 0.67484395\n",
      "Iteration 422, loss = 0.67463149\n",
      "Iteration 423, loss = 0.67441938\n",
      "Iteration 424, loss = 0.67420761\n",
      "Iteration 425, loss = 0.67399618\n",
      "Iteration 426, loss = 0.67378510\n",
      "Iteration 427, loss = 0.67357435\n",
      "Iteration 428, loss = 0.67336393\n",
      "Iteration 429, loss = 0.67315384\n",
      "Iteration 430, loss = 0.67294409\n",
      "Iteration 431, loss = 0.67273466\n",
      "Iteration 432, loss = 0.67252556\n",
      "Iteration 433, loss = 0.67231678\n",
      "Iteration 434, loss = 0.67210832\n",
      "Iteration 435, loss = 0.67190018\n",
      "Iteration 436, loss = 0.67169236\n",
      "Iteration 437, loss = 0.67148485\n",
      "Iteration 438, loss = 0.67127765\n",
      "Iteration 439, loss = 0.67107076\n",
      "Iteration 440, loss = 0.67086419\n",
      "Iteration 441, loss = 0.67065791\n",
      "Iteration 442, loss = 0.67045194\n",
      "Iteration 443, loss = 0.67024628\n",
      "Iteration 444, loss = 0.67004091\n",
      "Iteration 445, loss = 0.66983584\n",
      "Iteration 446, loss = 0.66963106\n",
      "Iteration 447, loss = 0.66942658\n",
      "Iteration 448, loss = 0.66922239\n",
      "Iteration 449, loss = 0.66901849\n",
      "Iteration 450, loss = 0.66881488\n",
      "Iteration 451, loss = 0.66861155\n",
      "Iteration 452, loss = 0.66840850\n",
      "Iteration 453, loss = 0.66820574\n",
      "Iteration 454, loss = 0.66800326\n",
      "Iteration 455, loss = 0.66780105\n",
      "Iteration 456, loss = 0.66759912\n",
      "Iteration 457, loss = 0.66739746\n",
      "Iteration 458, loss = 0.66719607\n",
      "Iteration 459, loss = 0.66699496\n",
      "Iteration 460, loss = 0.66679411\n",
      "Iteration 461, loss = 0.66659353\n",
      "Iteration 462, loss = 0.66639321\n",
      "Iteration 463, loss = 0.66619315\n",
      "Iteration 464, loss = 0.66599335\n",
      "Iteration 465, loss = 0.66579381\n",
      "Iteration 466, loss = 0.66559453\n",
      "Iteration 467, loss = 0.66539550\n",
      "Iteration 468, loss = 0.66519673\n",
      "Iteration 469, loss = 0.66499821\n",
      "Iteration 470, loss = 0.66479993\n",
      "Iteration 471, loss = 0.66460191\n",
      "Iteration 472, loss = 0.66440412\n",
      "Iteration 473, loss = 0.66420659\n",
      "Iteration 474, loss = 0.66400929\n",
      "Iteration 475, loss = 0.66381224\n",
      "Iteration 476, loss = 0.66361542\n",
      "Iteration 477, loss = 0.66341884\n",
      "Iteration 478, loss = 0.66322250\n",
      "Iteration 479, loss = 0.66302639\n",
      "Iteration 480, loss = 0.66283051\n",
      "Iteration 481, loss = 0.66263487\n",
      "Iteration 482, loss = 0.66243945\n",
      "Iteration 483, loss = 0.66224426\n",
      "Iteration 484, loss = 0.66204929\n",
      "Iteration 485, loss = 0.66185455\n",
      "Iteration 486, loss = 0.66166002\n",
      "Iteration 487, loss = 0.66146572\n",
      "Iteration 488, loss = 0.66127164\n",
      "Iteration 489, loss = 0.66107778\n",
      "Iteration 490, loss = 0.66088413\n",
      "Iteration 491, loss = 0.66069069\n",
      "Iteration 492, loss = 0.66049747\n",
      "Iteration 493, loss = 0.66030446\n",
      "Iteration 494, loss = 0.66011166\n",
      "Iteration 495, loss = 0.65991906\n",
      "Iteration 496, loss = 0.65972667\n",
      "Iteration 497, loss = 0.65953449\n",
      "Iteration 498, loss = 0.65934251\n",
      "Iteration 499, loss = 0.65915073\n",
      "Iteration 500, loss = 0.65895916\n",
      "Iteration 501, loss = 0.65876778\n",
      "Iteration 502, loss = 0.65857660\n",
      "Iteration 503, loss = 0.65838561\n",
      "Iteration 504, loss = 0.65819482\n",
      "Iteration 505, loss = 0.65800422\n",
      "Iteration 506, loss = 0.65781382\n",
      "Iteration 507, loss = 0.65762360\n",
      "Iteration 508, loss = 0.65743358\n",
      "Iteration 509, loss = 0.65724374\n",
      "Iteration 510, loss = 0.65705408\n",
      "Iteration 511, loss = 0.65686462\n",
      "Iteration 512, loss = 0.65667533\n",
      "Iteration 513, loss = 0.65648623\n",
      "Iteration 514, loss = 0.65629731\n",
      "Iteration 515, loss = 0.65610857\n",
      "Iteration 516, loss = 0.65592000\n",
      "Iteration 517, loss = 0.65573161\n",
      "Iteration 518, loss = 0.65554340\n",
      "Iteration 519, loss = 0.65535537\n",
      "Iteration 520, loss = 0.65516750\n",
      "Iteration 521, loss = 0.65497981\n",
      "Iteration 522, loss = 0.65479229\n",
      "Iteration 523, loss = 0.65460493\n",
      "Iteration 524, loss = 0.65441775\n",
      "Iteration 525, loss = 0.65423073\n",
      "Iteration 526, loss = 0.65404388\n",
      "Iteration 527, loss = 0.65385719\n",
      "Iteration 528, loss = 0.65367066\n",
      "Iteration 529, loss = 0.65348430\n",
      "Iteration 530, loss = 0.65329809\n",
      "Iteration 531, loss = 0.65311205\n",
      "Iteration 532, loss = 0.65292616\n",
      "Iteration 533, loss = 0.65274044\n",
      "Iteration 534, loss = 0.65255486\n",
      "Iteration 535, loss = 0.65236945\n",
      "Iteration 536, loss = 0.65218418\n",
      "Iteration 537, loss = 0.65199907\n",
      "Iteration 538, loss = 0.65181411\n",
      "Iteration 539, loss = 0.65162930\n",
      "Iteration 540, loss = 0.65144464\n",
      "Iteration 541, loss = 0.65126013\n",
      "Iteration 542, loss = 0.65107577\n",
      "Iteration 543, loss = 0.65089155\n",
      "Iteration 544, loss = 0.65070748\n",
      "Iteration 545, loss = 0.65052355\n",
      "Iteration 546, loss = 0.65033976\n",
      "Iteration 547, loss = 0.65015612\n",
      "Iteration 548, loss = 0.64997262\n",
      "Iteration 549, loss = 0.64978925\n",
      "Iteration 550, loss = 0.64960603\n",
      "Iteration 551, loss = 0.64942294\n",
      "Iteration 552, loss = 0.64923999\n",
      "Iteration 553, loss = 0.64905718\n",
      "Iteration 554, loss = 0.64887450\n",
      "Iteration 555, loss = 0.64869195\n",
      "Iteration 556, loss = 0.64850954\n",
      "Iteration 557, loss = 0.64832726\n",
      "Iteration 558, loss = 0.64814511\n",
      "Iteration 559, loss = 0.64796310\n",
      "Iteration 560, loss = 0.64778121\n",
      "Iteration 561, loss = 0.64759945\n",
      "Iteration 562, loss = 0.64741781\n",
      "Iteration 563, loss = 0.64723630\n",
      "Iteration 564, loss = 0.64705492\n",
      "Iteration 565, loss = 0.64687367\n",
      "Iteration 566, loss = 0.64669253\n",
      "Iteration 567, loss = 0.64651152\n",
      "Iteration 568, loss = 0.64633064\n",
      "Iteration 569, loss = 0.64614987\n",
      "Iteration 570, loss = 0.64596922\n",
      "Iteration 571, loss = 0.64578870\n",
      "Iteration 572, loss = 0.64560829\n",
      "Iteration 573, loss = 0.64542800\n",
      "Iteration 574, loss = 0.64524782\n",
      "Iteration 575, loss = 0.64506777\n",
      "Iteration 576, loss = 0.64488783\n",
      "Iteration 577, loss = 0.64470800\n",
      "Iteration 578, loss = 0.64452829\n",
      "Iteration 579, loss = 0.64434869\n",
      "Iteration 580, loss = 0.64416920\n",
      "Iteration 581, loss = 0.64398982\n",
      "Iteration 582, loss = 0.64381056\n",
      "Iteration 583, loss = 0.64363140\n",
      "Iteration 584, loss = 0.64345236\n",
      "Iteration 585, loss = 0.64327342\n",
      "Iteration 586, loss = 0.64309459\n",
      "Iteration 587, loss = 0.64291587\n",
      "Iteration 588, loss = 0.64273725\n",
      "Iteration 589, loss = 0.64255874\n",
      "Iteration 590, loss = 0.64238033\n",
      "Iteration 591, loss = 0.64220203\n",
      "Iteration 592, loss = 0.64202383\n",
      "Iteration 593, loss = 0.64184574\n",
      "Iteration 594, loss = 0.64166775\n",
      "Iteration 595, loss = 0.64148985\n",
      "Iteration 596, loss = 0.64131206\n",
      "Iteration 597, loss = 0.64113437\n",
      "Iteration 598, loss = 0.64095678\n",
      "Iteration 599, loss = 0.64077929\n",
      "Iteration 600, loss = 0.64060189\n",
      "Iteration 601, loss = 0.64042460\n",
      "Iteration 602, loss = 0.64024740\n",
      "Iteration 603, loss = 0.64007029\n",
      "Iteration 604, loss = 0.63989328\n",
      "Iteration 605, loss = 0.63971637\n",
      "Iteration 606, loss = 0.63953955\n",
      "Iteration 607, loss = 0.63936283\n",
      "Iteration 608, loss = 0.63918619\n",
      "Iteration 609, loss = 0.63900966\n",
      "Iteration 610, loss = 0.63883321\n",
      "Iteration 611, loss = 0.63865685\n",
      "Iteration 612, loss = 0.63848059\n",
      "Iteration 613, loss = 0.63830441\n",
      "Iteration 614, loss = 0.63812833\n",
      "Iteration 615, loss = 0.63795233\n",
      "Iteration 616, loss = 0.63777642\n",
      "Iteration 617, loss = 0.63760060\n",
      "Iteration 618, loss = 0.63742487\n",
      "Iteration 619, loss = 0.63724923\n",
      "Iteration 620, loss = 0.63707367\n",
      "Iteration 621, loss = 0.63689820\n",
      "Iteration 622, loss = 0.63672281\n",
      "Iteration 623, loss = 0.63654751\n",
      "Iteration 624, loss = 0.63637229\n",
      "Iteration 625, loss = 0.63619716\n",
      "Iteration 626, loss = 0.63602211\n",
      "Iteration 627, loss = 0.63584714\n",
      "Iteration 628, loss = 0.63567225\n",
      "Iteration 629, loss = 0.63549745\n",
      "Iteration 630, loss = 0.63532273\n",
      "Iteration 631, loss = 0.63514809\n",
      "Iteration 632, loss = 0.63497353\n",
      "Iteration 633, loss = 0.63479904\n",
      "Iteration 634, loss = 0.63462464\n",
      "Iteration 635, loss = 0.63445032\n",
      "Iteration 636, loss = 0.63427608\n",
      "Iteration 637, loss = 0.63410191\n",
      "Iteration 638, loss = 0.63392783\n",
      "Iteration 639, loss = 0.63375381\n",
      "Iteration 640, loss = 0.63357988\n",
      "Iteration 641, loss = 0.63340602\n",
      "Iteration 642, loss = 0.63323224\n",
      "Iteration 643, loss = 0.63305854\n",
      "Iteration 644, loss = 0.63288491\n",
      "Iteration 645, loss = 0.63271135\n",
      "Iteration 646, loss = 0.63253787\n",
      "Iteration 647, loss = 0.63236446\n",
      "Iteration 648, loss = 0.63219113\n",
      "Iteration 649, loss = 0.63201787\n",
      "Iteration 650, loss = 0.63184468\n",
      "Iteration 651, loss = 0.63167156\n",
      "Iteration 652, loss = 0.63149852\n",
      "Iteration 653, loss = 0.63132555\n",
      "Iteration 654, loss = 0.63115265\n",
      "Iteration 655, loss = 0.63097982\n",
      "Iteration 656, loss = 0.63080706\n",
      "Iteration 657, loss = 0.63063437\n",
      "Iteration 658, loss = 0.63046175\n",
      "Iteration 659, loss = 0.63028920\n",
      "Iteration 660, loss = 0.63011672\n",
      "Iteration 661, loss = 0.62994430\n",
      "Iteration 662, loss = 0.62977196\n",
      "Iteration 663, loss = 0.62959968\n",
      "Iteration 664, loss = 0.62942747\n",
      "Iteration 665, loss = 0.62925533\n",
      "Iteration 666, loss = 0.62908326\n",
      "Iteration 667, loss = 0.62891125\n",
      "Iteration 668, loss = 0.62873931\n",
      "Iteration 669, loss = 0.62856743\n",
      "Iteration 670, loss = 0.62839562\n",
      "Iteration 671, loss = 0.62822388\n",
      "Iteration 672, loss = 0.62805220\n",
      "Iteration 673, loss = 0.62788059\n",
      "Iteration 674, loss = 0.62770904\n",
      "Iteration 675, loss = 0.62753755\n",
      "Iteration 676, loss = 0.62736613\n",
      "Iteration 677, loss = 0.62719477\n",
      "Iteration 678, loss = 0.62702348\n",
      "Iteration 679, loss = 0.62685225\n",
      "Iteration 680, loss = 0.62668108\n",
      "Iteration 681, loss = 0.62650997\n",
      "Iteration 682, loss = 0.62633893\n",
      "Iteration 683, loss = 0.62616794\n",
      "Iteration 684, loss = 0.62599702\n",
      "Iteration 685, loss = 0.62582616\n",
      "Iteration 686, loss = 0.62565536\n",
      "Iteration 687, loss = 0.62548463\n",
      "Iteration 688, loss = 0.62531395\n",
      "Iteration 689, loss = 0.62514333\n",
      "Iteration 690, loss = 0.62497278\n",
      "Iteration 691, loss = 0.62480228\n",
      "Iteration 692, loss = 0.62463184\n",
      "Iteration 693, loss = 0.62446146\n",
      "Iteration 694, loss = 0.62429115\n",
      "Iteration 695, loss = 0.62412089\n",
      "Iteration 696, loss = 0.62395068\n",
      "Iteration 697, loss = 0.62378054\n",
      "Iteration 698, loss = 0.62361046\n",
      "Iteration 699, loss = 0.62344043\n",
      "Iteration 700, loss = 0.62327046\n",
      "Iteration 701, loss = 0.62310055\n",
      "Iteration 702, loss = 0.62293069\n",
      "Iteration 703, loss = 0.62276090\n",
      "Iteration 704, loss = 0.62259116\n",
      "Iteration 705, loss = 0.62242147\n",
      "Iteration 706, loss = 0.62225185\n",
      "Iteration 707, loss = 0.62208227\n",
      "Iteration 708, loss = 0.62191276\n",
      "Iteration 709, loss = 0.62174330\n",
      "Iteration 710, loss = 0.62157389\n",
      "Iteration 711, loss = 0.62140455\n",
      "Iteration 712, loss = 0.62123525\n",
      "Iteration 713, loss = 0.62106601\n",
      "Iteration 714, loss = 0.62089683\n",
      "Iteration 715, loss = 0.62072770\n",
      "Iteration 716, loss = 0.62055863\n",
      "Iteration 717, loss = 0.62038961\n",
      "Iteration 718, loss = 0.62022064\n",
      "Iteration 719, loss = 0.62005173\n",
      "Iteration 720, loss = 0.61988287\n",
      "Iteration 721, loss = 0.61971406\n",
      "Iteration 722, loss = 0.61954531\n",
      "Iteration 723, loss = 0.61937661\n",
      "Iteration 724, loss = 0.61920797\n",
      "Iteration 725, loss = 0.61903937\n",
      "Iteration 726, loss = 0.61887083\n",
      "Iteration 727, loss = 0.61870234\n",
      "Iteration 728, loss = 0.61853391\n",
      "Iteration 729, loss = 0.61836553\n",
      "Iteration 730, loss = 0.61819719\n",
      "Iteration 731, loss = 0.61802892\n",
      "Iteration 732, loss = 0.61786069\n",
      "Iteration 733, loss = 0.61769251\n",
      "Iteration 734, loss = 0.61752439\n",
      "Iteration 735, loss = 0.61735631\n",
      "Iteration 736, loss = 0.61718829\n",
      "Iteration 737, loss = 0.61702032\n",
      "Iteration 738, loss = 0.61685240\n",
      "Iteration 739, loss = 0.61668453\n",
      "Iteration 740, loss = 0.61651671\n",
      "Iteration 741, loss = 0.61634894\n",
      "Iteration 742, loss = 0.61618122\n",
      "Iteration 743, loss = 0.61601355\n",
      "Iteration 744, loss = 0.61584593\n",
      "Iteration 745, loss = 0.61567836\n",
      "Iteration 746, loss = 0.61551085\n",
      "Iteration 747, loss = 0.61534338\n",
      "Iteration 748, loss = 0.61517596\n",
      "Iteration 749, loss = 0.61500859\n",
      "Iteration 750, loss = 0.61484126\n",
      "Iteration 751, loss = 0.61467399\n",
      "Iteration 752, loss = 0.61450677\n",
      "Iteration 753, loss = 0.61433959\n",
      "Iteration 754, loss = 0.61417247\n",
      "Iteration 755, loss = 0.61400539\n",
      "Iteration 756, loss = 0.61383836\n",
      "Iteration 757, loss = 0.61367138\n",
      "Iteration 758, loss = 0.61350445\n",
      "Iteration 759, loss = 0.61333757\n",
      "Iteration 760, loss = 0.61317073\n",
      "Iteration 761, loss = 0.61300394\n",
      "Iteration 762, loss = 0.61283720\n",
      "Iteration 763, loss = 0.61267051\n",
      "Iteration 764, loss = 0.61250387\n",
      "Iteration 765, loss = 0.61233727\n",
      "Iteration 766, loss = 0.61217072\n",
      "Iteration 767, loss = 0.61200422\n",
      "Iteration 768, loss = 0.61183776\n",
      "Iteration 769, loss = 0.61167135\n",
      "Iteration 770, loss = 0.61150499\n",
      "Iteration 771, loss = 0.61133868\n",
      "Iteration 772, loss = 0.61117241\n",
      "Iteration 773, loss = 0.61100619\n",
      "Iteration 774, loss = 0.61084002\n",
      "Iteration 775, loss = 0.61067389\n",
      "Iteration 776, loss = 0.61050781\n",
      "Iteration 777, loss = 0.61034178\n",
      "Iteration 778, loss = 0.61017579\n",
      "Iteration 779, loss = 0.61000985\n",
      "Iteration 780, loss = 0.60984395\n",
      "Iteration 781, loss = 0.60967810\n",
      "Iteration 782, loss = 0.60951230\n",
      "Iteration 783, loss = 0.60934654\n",
      "Iteration 784, loss = 0.60918083\n",
      "Iteration 785, loss = 0.60901517\n",
      "Iteration 786, loss = 0.60884955\n",
      "Iteration 787, loss = 0.60868397\n",
      "Iteration 788, loss = 0.60851844\n",
      "Iteration 789, loss = 0.60835296\n",
      "Iteration 790, loss = 0.60818752\n",
      "Iteration 791, loss = 0.60802213\n",
      "Iteration 792, loss = 0.60785678\n",
      "Iteration 793, loss = 0.60769148\n",
      "Iteration 794, loss = 0.60752622\n",
      "Iteration 795, loss = 0.60736101\n",
      "Iteration 796, loss = 0.60719584\n",
      "Iteration 797, loss = 0.60703072\n",
      "Iteration 798, loss = 0.60686564\n",
      "Iteration 799, loss = 0.60670061\n",
      "Iteration 800, loss = 0.60653562\n",
      "Iteration 801, loss = 0.60637068\n",
      "Iteration 802, loss = 0.60620578\n",
      "Iteration 803, loss = 0.60604093\n",
      "Iteration 804, loss = 0.60587612\n",
      "Iteration 805, loss = 0.60571135\n",
      "Iteration 806, loss = 0.60554663\n",
      "Iteration 807, loss = 0.60538195\n",
      "Iteration 808, loss = 0.60521732\n",
      "Iteration 809, loss = 0.60505273\n",
      "Iteration 810, loss = 0.60488819\n",
      "Iteration 811, loss = 0.60472369\n",
      "Iteration 812, loss = 0.60455923\n",
      "Iteration 813, loss = 0.60439482\n",
      "Iteration 814, loss = 0.60423045\n",
      "Iteration 815, loss = 0.60406612\n",
      "Iteration 816, loss = 0.60390184\n",
      "Iteration 817, loss = 0.60373761\n",
      "Iteration 818, loss = 0.60357341\n",
      "Iteration 819, loss = 0.60340926\n",
      "Iteration 820, loss = 0.60324516\n",
      "Iteration 821, loss = 0.60308109\n",
      "Iteration 822, loss = 0.60291707\n",
      "Iteration 823, loss = 0.60275310\n",
      "Iteration 824, loss = 0.60258917\n",
      "Iteration 825, loss = 0.60242528\n",
      "Iteration 826, loss = 0.60226143\n",
      "Iteration 827, loss = 0.60209763\n",
      "Iteration 828, loss = 0.60193387\n",
      "Iteration 829, loss = 0.60177015\n",
      "Iteration 830, loss = 0.60160648\n",
      "Iteration 831, loss = 0.60144285\n",
      "Iteration 832, loss = 0.60127927\n",
      "Iteration 833, loss = 0.60111572\n",
      "Iteration 834, loss = 0.60095222\n",
      "Iteration 835, loss = 0.60078876\n",
      "Iteration 836, loss = 0.60062535\n",
      "Iteration 837, loss = 0.60046198\n",
      "Iteration 838, loss = 0.60029865\n",
      "Iteration 839, loss = 0.60013536\n",
      "Iteration 840, loss = 0.59997212\n",
      "Iteration 841, loss = 0.59980892\n",
      "Iteration 842, loss = 0.59964576\n",
      "Iteration 843, loss = 0.59948264\n",
      "Iteration 844, loss = 0.59931957\n",
      "Iteration 845, loss = 0.59915654\n",
      "Iteration 846, loss = 0.59899355\n",
      "Iteration 847, loss = 0.59883061\n",
      "Iteration 848, loss = 0.59866771\n",
      "Iteration 849, loss = 0.59850485\n",
      "Iteration 850, loss = 0.59834203\n",
      "Iteration 851, loss = 0.59817925\n",
      "Iteration 852, loss = 0.59801652\n",
      "Iteration 853, loss = 0.59785383\n",
      "Iteration 854, loss = 0.59769118\n",
      "Iteration 855, loss = 0.59752858\n",
      "Iteration 856, loss = 0.59736601\n",
      "Iteration 857, loss = 0.59720349\n",
      "Iteration 858, loss = 0.59704101\n",
      "Iteration 859, loss = 0.59687858\n",
      "Iteration 860, loss = 0.59671618\n",
      "Iteration 861, loss = 0.59655383\n",
      "Iteration 862, loss = 0.59639152\n",
      "Iteration 863, loss = 0.59622925\n",
      "Iteration 864, loss = 0.59606702\n",
      "Iteration 865, loss = 0.59590484\n",
      "Iteration 866, loss = 0.59574270\n",
      "Iteration 867, loss = 0.59558060\n",
      "Iteration 868, loss = 0.59541854\n",
      "Iteration 869, loss = 0.59525652\n",
      "Iteration 870, loss = 0.59509455\n",
      "Iteration 871, loss = 0.59493262\n",
      "Iteration 872, loss = 0.59477073\n",
      "Iteration 873, loss = 0.59460888\n",
      "Iteration 874, loss = 0.59444707\n",
      "Iteration 875, loss = 0.59428531\n",
      "Iteration 876, loss = 0.59412358\n",
      "Iteration 877, loss = 0.59396190\n",
      "Iteration 878, loss = 0.59380026\n",
      "Iteration 879, loss = 0.59363866\n",
      "Iteration 880, loss = 0.59347711\n",
      "Iteration 881, loss = 0.59331559\n",
      "Iteration 882, loss = 0.59315412\n",
      "Iteration 883, loss = 0.59299269\n",
      "Iteration 884, loss = 0.59283130\n",
      "Iteration 885, loss = 0.59266995\n",
      "Iteration 886, loss = 0.59250865\n",
      "Iteration 887, loss = 0.59234738\n",
      "Iteration 888, loss = 0.59218616\n",
      "Iteration 889, loss = 0.59202498\n",
      "Iteration 890, loss = 0.59186384\n",
      "Iteration 891, loss = 0.59170274\n",
      "Iteration 892, loss = 0.59154169\n",
      "Iteration 893, loss = 0.59138067\n",
      "Iteration 894, loss = 0.59121970\n",
      "Iteration 895, loss = 0.59105877\n",
      "Iteration 896, loss = 0.59089788\n",
      "Iteration 897, loss = 0.59073703\n",
      "Iteration 898, loss = 0.59057622\n",
      "Iteration 899, loss = 0.59041546\n",
      "Iteration 900, loss = 0.59025473\n",
      "Iteration 901, loss = 0.59009405\n",
      "Iteration 902, loss = 0.58993341\n",
      "Iteration 903, loss = 0.58977281\n",
      "Iteration 904, loss = 0.58961225\n",
      "Iteration 905, loss = 0.58945173\n",
      "Iteration 906, loss = 0.58929126\n",
      "Iteration 907, loss = 0.58913082\n",
      "Iteration 908, loss = 0.58897043\n",
      "Iteration 909, loss = 0.58881008\n",
      "Iteration 910, loss = 0.58864977\n",
      "Iteration 911, loss = 0.58848950\n",
      "Iteration 912, loss = 0.58832927\n",
      "Iteration 913, loss = 0.58816909\n",
      "Iteration 914, loss = 0.58800894\n",
      "Iteration 915, loss = 0.58784884\n",
      "Iteration 916, loss = 0.58768878\n",
      "Iteration 917, loss = 0.58752876\n",
      "Iteration 918, loss = 0.58736878\n",
      "Iteration 919, loss = 0.58720884\n",
      "Iteration 920, loss = 0.58704894\n",
      "Iteration 921, loss = 0.58688909\n",
      "Iteration 922, loss = 0.58672927\n",
      "Iteration 923, loss = 0.58656950\n",
      "Iteration 924, loss = 0.58640977\n",
      "Iteration 925, loss = 0.58625008\n",
      "Iteration 926, loss = 0.58609043\n",
      "Iteration 927, loss = 0.58593082\n",
      "Iteration 928, loss = 0.58577125\n",
      "Iteration 929, loss = 0.58561173\n",
      "Iteration 930, loss = 0.58545224\n",
      "Iteration 931, loss = 0.58529280\n",
      "Iteration 932, loss = 0.58513340\n",
      "Iteration 933, loss = 0.58497404\n",
      "Iteration 934, loss = 0.58481472\n",
      "Iteration 935, loss = 0.58465544\n",
      "Iteration 936, loss = 0.58449620\n",
      "Iteration 937, loss = 0.58433701\n",
      "Iteration 938, loss = 0.58417785\n",
      "Iteration 939, loss = 0.58401874\n",
      "Iteration 940, loss = 0.58385967\n",
      "Iteration 941, loss = 0.58370064\n",
      "Iteration 942, loss = 0.58354165\n",
      "Iteration 943, loss = 0.58338270\n",
      "Iteration 944, loss = 0.58322379\n",
      "Iteration 945, loss = 0.58306493\n",
      "Iteration 946, loss = 0.58290610\n",
      "Iteration 947, loss = 0.58274732\n",
      "Iteration 948, loss = 0.58258857\n",
      "Iteration 949, loss = 0.58242987\n",
      "Iteration 950, loss = 0.58227121\n",
      "Iteration 951, loss = 0.58211259\n",
      "Iteration 952, loss = 0.58195402\n",
      "Iteration 953, loss = 0.58179548\n",
      "Iteration 954, loss = 0.58163698\n",
      "Iteration 955, loss = 0.58147853\n",
      "Iteration 956, loss = 0.58132011\n",
      "Iteration 957, loss = 0.58116174\n",
      "Iteration 958, loss = 0.58100341\n",
      "Iteration 959, loss = 0.58084512\n",
      "Iteration 960, loss = 0.58068687\n",
      "Iteration 961, loss = 0.58052867\n",
      "Iteration 962, loss = 0.58037050\n",
      "Iteration 963, loss = 0.58021237\n",
      "Iteration 964, loss = 0.58005429\n",
      "Iteration 965, loss = 0.57989625\n",
      "Iteration 966, loss = 0.57973824\n",
      "Iteration 967, loss = 0.57958028\n",
      "Iteration 968, loss = 0.57942236\n",
      "Iteration 969, loss = 0.57926449\n",
      "Iteration 970, loss = 0.57910665\n",
      "Iteration 971, loss = 0.57894885\n",
      "Iteration 972, loss = 0.57879110\n",
      "Iteration 973, loss = 0.57863339\n",
      "Iteration 974, loss = 0.57847571\n",
      "Iteration 975, loss = 0.57831808\n",
      "Iteration 976, loss = 0.57816049\n",
      "Iteration 977, loss = 0.57800294\n",
      "Iteration 978, loss = 0.57784543\n",
      "Iteration 979, loss = 0.57768797\n",
      "Iteration 980, loss = 0.57753054\n",
      "Iteration 981, loss = 0.57737316\n",
      "Iteration 982, loss = 0.57721582\n",
      "Iteration 983, loss = 0.57705851\n",
      "Iteration 984, loss = 0.57690125\n",
      "Iteration 985, loss = 0.57674403\n",
      "Iteration 986, loss = 0.57658686\n",
      "Iteration 987, loss = 0.57642972\n",
      "Iteration 988, loss = 0.57627262\n",
      "Iteration 989, loss = 0.57611557\n",
      "Iteration 990, loss = 0.57595855\n",
      "Iteration 991, loss = 0.57580158\n",
      "Iteration 992, loss = 0.57564465\n",
      "Iteration 993, loss = 0.57548776\n",
      "Iteration 994, loss = 0.57533091\n",
      "Iteration 995, loss = 0.57517411\n",
      "Iteration 996, loss = 0.57501734\n",
      "Iteration 997, loss = 0.57486061\n",
      "Iteration 998, loss = 0.57470393\n",
      "Iteration 999, loss = 0.57454729\n",
      "Iteration 1000, loss = 0.57439069\n",
      "Iteration 1001, loss = 0.57423413\n",
      "Iteration 1002, loss = 0.57407761\n",
      "Iteration 1003, loss = 0.57392113\n",
      "Iteration 1004, loss = 0.57376469\n",
      "Iteration 1005, loss = 0.57360830\n",
      "Iteration 1006, loss = 0.57345194\n",
      "Iteration 1007, loss = 0.57329563\n",
      "Iteration 1008, loss = 0.57313936\n",
      "Iteration 1009, loss = 0.57298313\n",
      "Iteration 1010, loss = 0.57282694\n",
      "Iteration 1011, loss = 0.57267079\n",
      "Iteration 1012, loss = 0.57251469\n",
      "Iteration 1013, loss = 0.57235862\n",
      "Iteration 1014, loss = 0.57220260\n",
      "Iteration 1015, loss = 0.57204662\n",
      "Iteration 1016, loss = 0.57189068\n",
      "Iteration 1017, loss = 0.57173478\n",
      "Iteration 1018, loss = 0.57157892\n",
      "Iteration 1019, loss = 0.57142310\n",
      "Iteration 1020, loss = 0.57126732\n",
      "Iteration 1021, loss = 0.57111159\n",
      "Iteration 1022, loss = 0.57095590\n",
      "Iteration 1023, loss = 0.57080024\n",
      "Iteration 1024, loss = 0.57064463\n",
      "Iteration 1025, loss = 0.57048907\n",
      "Iteration 1026, loss = 0.57033354\n",
      "Iteration 1027, loss = 0.57017805\n",
      "Iteration 1028, loss = 0.57002261\n",
      "Iteration 1029, loss = 0.56986720\n",
      "Iteration 1030, loss = 0.56971184\n",
      "Iteration 1031, loss = 0.56955652\n",
      "Iteration 1032, loss = 0.56940124\n",
      "Iteration 1033, loss = 0.56924600\n",
      "Iteration 1034, loss = 0.56909080\n",
      "Iteration 1035, loss = 0.56893565\n",
      "Iteration 1036, loss = 0.56878053\n",
      "Iteration 1037, loss = 0.56862546\n",
      "Iteration 1038, loss = 0.56847043\n",
      "Iteration 1039, loss = 0.56831544\n",
      "Iteration 1040, loss = 0.56816049\n",
      "Iteration 1041, loss = 0.56800558\n",
      "Iteration 1042, loss = 0.56785072\n",
      "Iteration 1043, loss = 0.56769589\n",
      "Iteration 1044, loss = 0.56754111\n",
      "Iteration 1045, loss = 0.56738637\n",
      "Iteration 1046, loss = 0.56723167\n",
      "Iteration 1047, loss = 0.56707701\n",
      "Iteration 1048, loss = 0.56692239\n",
      "Iteration 1049, loss = 0.56676782\n",
      "Iteration 1050, loss = 0.56661328\n",
      "Iteration 1051, loss = 0.56645879\n",
      "Iteration 1052, loss = 0.56630434\n",
      "Iteration 1053, loss = 0.56614993\n",
      "Iteration 1054, loss = 0.56599556\n",
      "Iteration 1055, loss = 0.56584123\n",
      "Iteration 1056, loss = 0.56568695\n",
      "Iteration 1057, loss = 0.56553270\n",
      "Iteration 1058, loss = 0.56537850\n",
      "Iteration 1059, loss = 0.56522434\n",
      "Iteration 1060, loss = 0.56507022\n",
      "Iteration 1061, loss = 0.56491614\n",
      "Iteration 1062, loss = 0.56476211\n",
      "Iteration 1063, loss = 0.56460811\n",
      "Iteration 1064, loss = 0.56445416\n",
      "Iteration 1065, loss = 0.56430025\n",
      "Iteration 1066, loss = 0.56414638\n",
      "Iteration 1067, loss = 0.56399255\n",
      "Iteration 1068, loss = 0.56383876\n",
      "Iteration 1069, loss = 0.56368502\n",
      "Iteration 1070, loss = 0.56353131\n",
      "Iteration 1071, loss = 0.56337765\n",
      "Iteration 1072, loss = 0.56322403\n",
      "Iteration 1073, loss = 0.56307045\n",
      "Iteration 1074, loss = 0.56291691\n",
      "Iteration 1075, loss = 0.56276342\n",
      "Iteration 1076, loss = 0.56260996\n",
      "Iteration 1077, loss = 0.56245655\n",
      "Iteration 1078, loss = 0.56230318\n",
      "Iteration 1079, loss = 0.56214985\n",
      "Iteration 1080, loss = 0.56199656\n",
      "Iteration 1081, loss = 0.56184332\n",
      "Iteration 1082, loss = 0.56169011\n",
      "Iteration 1083, loss = 0.56153695\n",
      "Iteration 1084, loss = 0.56138383\n",
      "Iteration 1085, loss = 0.56123075\n",
      "Iteration 1086, loss = 0.56107771\n",
      "Iteration 1087, loss = 0.56092471\n",
      "Iteration 1088, loss = 0.56077176\n",
      "Iteration 1089, loss = 0.56061885\n",
      "Iteration 1090, loss = 0.56046598\n",
      "Iteration 1091, loss = 0.56031315\n",
      "Iteration 1092, loss = 0.56016036\n",
      "Iteration 1093, loss = 0.56000761\n",
      "Iteration 1094, loss = 0.55985491\n",
      "Iteration 1095, loss = 0.55970225\n",
      "Iteration 1096, loss = 0.55954963\n",
      "Iteration 1097, loss = 0.55939705\n",
      "Iteration 1098, loss = 0.55924451\n",
      "Iteration 1099, loss = 0.55909201\n",
      "Iteration 1100, loss = 0.55893956\n",
      "Iteration 1101, loss = 0.55878715\n",
      "Iteration 1102, loss = 0.55863478\n",
      "Iteration 1103, loss = 0.55848245\n",
      "Iteration 1104, loss = 0.55833016\n",
      "Iteration 1105, loss = 0.55817792\n",
      "Iteration 1106, loss = 0.55802571\n",
      "Iteration 1107, loss = 0.55787355\n",
      "Iteration 1108, loss = 0.55772143\n",
      "Iteration 1109, loss = 0.55756936\n",
      "Iteration 1110, loss = 0.55741732\n",
      "Iteration 1111, loss = 0.55726533\n",
      "Iteration 1112, loss = 0.55711337\n",
      "Iteration 1113, loss = 0.55696146\n",
      "Iteration 1114, loss = 0.55680960\n",
      "Iteration 1115, loss = 0.55665777\n",
      "Iteration 1116, loss = 0.55650599\n",
      "Iteration 1117, loss = 0.55635424\n",
      "Iteration 1118, loss = 0.55620254\n",
      "Iteration 1119, loss = 0.55605088\n",
      "Iteration 1120, loss = 0.55589927\n",
      "Iteration 1121, loss = 0.55574769\n",
      "Iteration 1122, loss = 0.55559616\n",
      "Iteration 1123, loss = 0.55544466\n",
      "Iteration 1124, loss = 0.55529322\n",
      "Iteration 1125, loss = 0.55514181\n",
      "Iteration 1126, loss = 0.55499044\n",
      "Iteration 1127, loss = 0.55483912\n",
      "Iteration 1128, loss = 0.55468784\n",
      "Iteration 1129, loss = 0.55453660\n",
      "Iteration 1130, loss = 0.55438540\n",
      "Iteration 1131, loss = 0.55423424\n",
      "Iteration 1132, loss = 0.55408313\n",
      "Iteration 1133, loss = 0.55393206\n",
      "Iteration 1134, loss = 0.55378103\n",
      "Iteration 1135, loss = 0.55363004\n",
      "Iteration 1136, loss = 0.55347909\n",
      "Iteration 1137, loss = 0.55332819\n",
      "Iteration 1138, loss = 0.55317733\n",
      "Iteration 1139, loss = 0.55302651\n",
      "Iteration 1140, loss = 0.55287573\n",
      "Iteration 1141, loss = 0.55272499\n",
      "Iteration 1142, loss = 0.55257430\n",
      "Iteration 1143, loss = 0.55242364\n",
      "Iteration 1144, loss = 0.55227303\n",
      "Iteration 1145, loss = 0.55212247\n",
      "Iteration 1146, loss = 0.55197194\n",
      "Iteration 1147, loss = 0.55182146\n",
      "Iteration 1148, loss = 0.55167101\n",
      "Iteration 1149, loss = 0.55152061\n",
      "Iteration 1150, loss = 0.55137026\n",
      "Iteration 1151, loss = 0.55121994\n",
      "Iteration 1152, loss = 0.55106967\n",
      "Iteration 1153, loss = 0.55091944\n",
      "Iteration 1154, loss = 0.55076925\n",
      "Iteration 1155, loss = 0.55061910\n",
      "Iteration 1156, loss = 0.55046899\n",
      "Iteration 1157, loss = 0.55031893\n",
      "Iteration 1158, loss = 0.55016891\n",
      "Iteration 1159, loss = 0.55001893\n",
      "Iteration 1160, loss = 0.54986899\n",
      "Iteration 1161, loss = 0.54971910\n",
      "Iteration 1162, loss = 0.54956925\n",
      "Iteration 1163, loss = 0.54941944\n",
      "Iteration 1164, loss = 0.54926967\n",
      "Iteration 1165, loss = 0.54911994\n",
      "Iteration 1166, loss = 0.54897026\n",
      "Iteration 1167, loss = 0.54882062\n",
      "Iteration 1168, loss = 0.54867102\n",
      "Iteration 1169, loss = 0.54852146\n",
      "Iteration 1170, loss = 0.54837194\n",
      "Iteration 1171, loss = 0.54822247\n",
      "Iteration 1172, loss = 0.54807304\n",
      "Iteration 1173, loss = 0.54792365\n",
      "Iteration 1174, loss = 0.54777431\n",
      "Iteration 1175, loss = 0.54762500\n",
      "Iteration 1176, loss = 0.54747574\n",
      "Iteration 1177, loss = 0.54732652\n",
      "Iteration 1178, loss = 0.54717734\n",
      "Iteration 1179, loss = 0.54702821\n",
      "Iteration 1180, loss = 0.54687911\n",
      "Iteration 1181, loss = 0.54673006\n",
      "Iteration 1182, loss = 0.54658105\n",
      "Iteration 1183, loss = 0.54643209\n",
      "Iteration 1184, loss = 0.54628316\n",
      "Iteration 1185, loss = 0.54613428\n",
      "Iteration 1186, loss = 0.54598544\n",
      "Iteration 1187, loss = 0.54583665\n",
      "Iteration 1188, loss = 0.54568789\n",
      "Iteration 1189, loss = 0.54553918\n",
      "Iteration 1190, loss = 0.54539051\n",
      "Iteration 1191, loss = 0.54524188\n",
      "Iteration 1192, loss = 0.54509329\n",
      "Iteration 1193, loss = 0.54494475\n",
      "Iteration 1194, loss = 0.54479625\n",
      "Iteration 1195, loss = 0.54464779\n",
      "Iteration 1196, loss = 0.54449937\n",
      "Iteration 1197, loss = 0.54435100\n",
      "Iteration 1198, loss = 0.54420267\n",
      "Iteration 1199, loss = 0.54405438\n",
      "Iteration 1200, loss = 0.54390613\n",
      "Iteration 1201, loss = 0.54375793\n",
      "Iteration 1202, loss = 0.54360977\n",
      "Iteration 1203, loss = 0.54346165\n",
      "Iteration 1204, loss = 0.54331357\n",
      "Iteration 1205, loss = 0.54316553\n",
      "Iteration 1206, loss = 0.54301754\n",
      "Iteration 1207, loss = 0.54286959\n",
      "Iteration 1208, loss = 0.54272168\n",
      "Iteration 1209, loss = 0.54257382\n",
      "Iteration 1210, loss = 0.54242599\n",
      "Iteration 1211, loss = 0.54227821\n",
      "Iteration 1212, loss = 0.54213047\n",
      "Iteration 1213, loss = 0.54198278\n",
      "Iteration 1214, loss = 0.54183512\n",
      "Iteration 1215, loss = 0.54168751\n",
      "Iteration 1216, loss = 0.54153994\n",
      "Iteration 1217, loss = 0.54139242\n",
      "Iteration 1218, loss = 0.54124493\n",
      "Iteration 1219, loss = 0.54109749\n",
      "Iteration 1220, loss = 0.54095009\n",
      "Iteration 1221, loss = 0.54080274\n",
      "Iteration 1222, loss = 0.54065542\n",
      "Iteration 1223, loss = 0.54050815\n",
      "Iteration 1224, loss = 0.54036092\n",
      "Iteration 1225, loss = 0.54021373\n",
      "Iteration 1226, loss = 0.54006659\n",
      "Iteration 1227, loss = 0.53991949\n",
      "Iteration 1228, loss = 0.53977243\n",
      "Iteration 1229, loss = 0.53962541\n",
      "Iteration 1230, loss = 0.53947844\n",
      "Iteration 1231, loss = 0.53933151\n",
      "Iteration 1232, loss = 0.53918462\n",
      "Iteration 1233, loss = 0.53903777\n",
      "Iteration 1234, loss = 0.53889097\n",
      "Iteration 1235, loss = 0.53874420\n",
      "Iteration 1236, loss = 0.53859749\n",
      "Iteration 1237, loss = 0.53845081\n",
      "Iteration 1238, loss = 0.53830417\n",
      "Iteration 1239, loss = 0.53815758\n",
      "Iteration 1240, loss = 0.53801103\n",
      "Iteration 1241, loss = 0.53786453\n",
      "Iteration 1242, loss = 0.53771806\n",
      "Iteration 1243, loss = 0.53757164\n",
      "Iteration 1244, loss = 0.53742526\n",
      "Iteration 1245, loss = 0.53727893\n",
      "Iteration 1246, loss = 0.53713263\n",
      "Iteration 1247, loss = 0.53698638\n",
      "Iteration 1248, loss = 0.53684017\n",
      "Iteration 1249, loss = 0.53669401\n",
      "Iteration 1250, loss = 0.53654788\n",
      "Iteration 1251, loss = 0.53640180\n",
      "Iteration 1252, loss = 0.53625577\n",
      "Iteration 1253, loss = 0.53610977\n",
      "Iteration 1254, loss = 0.53596382\n",
      "Iteration 1255, loss = 0.53581791\n",
      "Iteration 1256, loss = 0.53567204\n",
      "Iteration 1257, loss = 0.53552621\n",
      "Iteration 1258, loss = 0.53538043\n",
      "Iteration 1259, loss = 0.53523469\n",
      "Iteration 1260, loss = 0.53508899\n",
      "Iteration 1261, loss = 0.53494334\n",
      "Iteration 1262, loss = 0.53479773\n",
      "Iteration 1263, loss = 0.53465216\n",
      "Iteration 1264, loss = 0.53450663\n",
      "Iteration 1265, loss = 0.53436115\n",
      "Iteration 1266, loss = 0.53421571\n",
      "Iteration 1267, loss = 0.53407031\n",
      "Iteration 1268, loss = 0.53392495\n",
      "Iteration 1269, loss = 0.53377964\n",
      "Iteration 1270, loss = 0.53363437\n",
      "Iteration 1271, loss = 0.53348914\n",
      "Iteration 1272, loss = 0.53334395\n",
      "Iteration 1273, loss = 0.53319881\n",
      "Iteration 1274, loss = 0.53305371\n",
      "Iteration 1275, loss = 0.53290865\n",
      "Iteration 1276, loss = 0.53276364\n",
      "Iteration 1277, loss = 0.53261867\n",
      "Iteration 1278, loss = 0.53247374\n",
      "Iteration 1279, loss = 0.53232885\n",
      "Iteration 1280, loss = 0.53218401\n",
      "Iteration 1281, loss = 0.53203921\n",
      "Iteration 1282, loss = 0.53189445\n",
      "Iteration 1283, loss = 0.53174973\n",
      "Iteration 1284, loss = 0.53160506\n",
      "Iteration 1285, loss = 0.53146043\n",
      "Iteration 1286, loss = 0.53131584\n",
      "Iteration 1287, loss = 0.53117130\n",
      "Iteration 1288, loss = 0.53102679\n",
      "Iteration 1289, loss = 0.53088233\n",
      "Iteration 1290, loss = 0.53073792\n",
      "Iteration 1291, loss = 0.53059354\n",
      "Iteration 1292, loss = 0.53044921\n",
      "Iteration 1293, loss = 0.53030492\n",
      "Iteration 1294, loss = 0.53016068\n",
      "Iteration 1295, loss = 0.53001648\n",
      "Iteration 1296, loss = 0.52987231\n",
      "Iteration 1297, loss = 0.52972820\n",
      "Iteration 1298, loss = 0.52958412\n",
      "Iteration 1299, loss = 0.52944009\n",
      "Iteration 1300, loss = 0.52929610\n",
      "Iteration 1301, loss = 0.52915216\n",
      "Iteration 1302, loss = 0.52900825\n",
      "Iteration 1303, loss = 0.52886439\n",
      "Iteration 1304, loss = 0.52872057\n",
      "Iteration 1305, loss = 0.52857680\n",
      "Iteration 1306, loss = 0.52843306\n",
      "Iteration 1307, loss = 0.52828937\n",
      "Iteration 1308, loss = 0.52814573\n",
      "Iteration 1309, loss = 0.52800212\n",
      "Iteration 1310, loss = 0.52785856\n",
      "Iteration 1311, loss = 0.52771504\n",
      "Iteration 1312, loss = 0.52757157\n",
      "Iteration 1313, loss = 0.52742813\n",
      "Iteration 1314, loss = 0.52728474\n",
      "Iteration 1315, loss = 0.52714140\n",
      "Iteration 1316, loss = 0.52699809\n",
      "Iteration 1317, loss = 0.52685483\n",
      "Iteration 1318, loss = 0.52671161\n",
      "Iteration 1319, loss = 0.52656844\n",
      "Iteration 1320, loss = 0.52642530\n",
      "Iteration 1321, loss = 0.52628221\n",
      "Iteration 1322, loss = 0.52613916\n",
      "Iteration 1323, loss = 0.52599616\n",
      "Iteration 1324, loss = 0.52585320\n",
      "Iteration 1325, loss = 0.52571028\n",
      "Iteration 1326, loss = 0.52556740\n",
      "Iteration 1327, loss = 0.52542457\n",
      "Iteration 1328, loss = 0.52528178\n",
      "Iteration 1329, loss = 0.52513903\n",
      "Iteration 1330, loss = 0.52499633\n",
      "Iteration 1331, loss = 0.52485366\n",
      "Iteration 1332, loss = 0.52471104\n",
      "Iteration 1333, loss = 0.52456847\n",
      "Iteration 1334, loss = 0.52442593\n",
      "Iteration 1335, loss = 0.52428344\n",
      "Iteration 1336, loss = 0.52414100\n",
      "Iteration 1337, loss = 0.52399859\n",
      "Iteration 1338, loss = 0.52385623\n",
      "Iteration 1339, loss = 0.52371391\n",
      "Iteration 1340, loss = 0.52357163\n",
      "Iteration 1341, loss = 0.52342940\n",
      "Iteration 1342, loss = 0.52328721\n",
      "Iteration 1343, loss = 0.52314506\n",
      "Iteration 1344, loss = 0.52300296\n",
      "Iteration 1345, loss = 0.52286089\n",
      "Iteration 1346, loss = 0.52271887\n",
      "Iteration 1347, loss = 0.52257690\n",
      "Iteration 1348, loss = 0.52243496\n",
      "Iteration 1349, loss = 0.52229307\n",
      "Iteration 1350, loss = 0.52215123\n",
      "Iteration 1351, loss = 0.52200942\n",
      "Iteration 1352, loss = 0.52186766\n",
      "Iteration 1353, loss = 0.52172594\n",
      "Iteration 1354, loss = 0.52158426\n",
      "Iteration 1355, loss = 0.52144263\n",
      "Iteration 1356, loss = 0.52130104\n",
      "Iteration 1357, loss = 0.52115949\n",
      "Iteration 1358, loss = 0.52101799\n",
      "Iteration 1359, loss = 0.52087653\n",
      "Iteration 1360, loss = 0.52073511\n",
      "Iteration 1361, loss = 0.52059373\n",
      "Iteration 1362, loss = 0.52045240\n",
      "Iteration 1363, loss = 0.52031111\n",
      "Iteration 1364, loss = 0.52016986\n",
      "Iteration 1365, loss = 0.52002866\n",
      "Iteration 1366, loss = 0.51988750\n",
      "Iteration 1367, loss = 0.51974638\n",
      "Iteration 1368, loss = 0.51960530\n",
      "Iteration 1369, loss = 0.51946427\n",
      "Iteration 1370, loss = 0.51932328\n",
      "Iteration 1371, loss = 0.51918233\n",
      "Iteration 1372, loss = 0.51904143\n",
      "Iteration 1373, loss = 0.51890057\n",
      "Iteration 1374, loss = 0.51875975\n",
      "Iteration 1375, loss = 0.51861897\n",
      "Iteration 1376, loss = 0.51847824\n",
      "Iteration 1377, loss = 0.51833755\n",
      "Iteration 1378, loss = 0.51819691\n",
      "Iteration 1379, loss = 0.51805630\n",
      "Iteration 1380, loss = 0.51791574\n",
      "Iteration 1381, loss = 0.51777522\n",
      "Iteration 1382, loss = 0.51763475\n",
      "Iteration 1383, loss = 0.51749432\n",
      "Iteration 1384, loss = 0.51735393\n",
      "Iteration 1385, loss = 0.51721358\n",
      "Iteration 1386, loss = 0.51707328\n",
      "Iteration 1387, loss = 0.51693302\n",
      "Iteration 1388, loss = 0.51679280\n",
      "Iteration 1389, loss = 0.51665263\n",
      "Iteration 1390, loss = 0.51651250\n",
      "Iteration 1391, loss = 0.51637241\n",
      "Iteration 1392, loss = 0.51623236\n",
      "Iteration 1393, loss = 0.51609236\n",
      "Iteration 1394, loss = 0.51595240\n",
      "Iteration 1395, loss = 0.51581248\n",
      "Iteration 1396, loss = 0.51567261\n",
      "Iteration 1397, loss = 0.51553278\n",
      "Iteration 1398, loss = 0.51539299\n",
      "Iteration 1399, loss = 0.51525324\n",
      "Iteration 1400, loss = 0.51511354\n",
      "Iteration 1401, loss = 0.51497388\n",
      "Iteration 1402, loss = 0.51483427\n",
      "Iteration 1403, loss = 0.51469469\n",
      "Iteration 1404, loss = 0.51455516\n",
      "Iteration 1405, loss = 0.51441568\n",
      "Iteration 1406, loss = 0.51427623\n",
      "Iteration 1407, loss = 0.51413683\n",
      "Iteration 1408, loss = 0.51399747\n",
      "Iteration 1409, loss = 0.51385815\n",
      "Iteration 1410, loss = 0.51371888\n",
      "Iteration 1411, loss = 0.51357965\n",
      "Iteration 1412, loss = 0.51344046\n",
      "Iteration 1413, loss = 0.51330132\n",
      "Iteration 1414, loss = 0.51316222\n",
      "Iteration 1415, loss = 0.51302316\n",
      "Iteration 1416, loss = 0.51288415\n",
      "Iteration 1417, loss = 0.51274517\n",
      "Iteration 1418, loss = 0.51260624\n",
      "Iteration 1419, loss = 0.51246736\n",
      "Iteration 1420, loss = 0.51232851\n",
      "Iteration 1421, loss = 0.51218971\n",
      "Iteration 1422, loss = 0.51205096\n",
      "Iteration 1423, loss = 0.51191224\n",
      "Iteration 1424, loss = 0.51177357\n",
      "Iteration 1425, loss = 0.51163494\n",
      "Iteration 1426, loss = 0.51149635\n",
      "Iteration 1427, loss = 0.51135781\n",
      "Iteration 1428, loss = 0.51121931\n",
      "Iteration 1429, loss = 0.51108085\n",
      "Iteration 1430, loss = 0.51094244\n",
      "Iteration 1431, loss = 0.51080407\n",
      "Iteration 1432, loss = 0.51066574\n",
      "Iteration 1433, loss = 0.51052745\n",
      "Iteration 1434, loss = 0.51038921\n",
      "Iteration 1435, loss = 0.51025101\n",
      "Iteration 1436, loss = 0.51011285\n",
      "Iteration 1437, loss = 0.50997474\n",
      "Iteration 1438, loss = 0.50983667\n",
      "Iteration 1439, loss = 0.50969864\n",
      "Iteration 1440, loss = 0.50956065\n",
      "Iteration 1441, loss = 0.50942271\n",
      "Iteration 1442, loss = 0.50928481\n",
      "Iteration 1443, loss = 0.50914696\n",
      "Iteration 1444, loss = 0.50900914\n",
      "Iteration 1445, loss = 0.50887137\n",
      "Iteration 1446, loss = 0.50873364\n",
      "Iteration 1447, loss = 0.50859596\n",
      "Iteration 1448, loss = 0.50845832\n",
      "Iteration 1449, loss = 0.50832072\n",
      "Iteration 1450, loss = 0.50818316\n",
      "Iteration 1451, loss = 0.50804565\n",
      "Iteration 1452, loss = 0.50790818\n",
      "Iteration 1453, loss = 0.50777075\n",
      "Iteration 1454, loss = 0.50763337\n",
      "Iteration 1455, loss = 0.50749603\n",
      "Iteration 1456, loss = 0.50735873\n",
      "Iteration 1457, loss = 0.50722147\n",
      "Iteration 1458, loss = 0.50708426\n",
      "Iteration 1459, loss = 0.50694709\n",
      "Iteration 1460, loss = 0.50680996\n",
      "Iteration 1461, loss = 0.50667288\n",
      "Iteration 1462, loss = 0.50653584\n",
      "Iteration 1463, loss = 0.50639884\n",
      "Iteration 1464, loss = 0.50626188\n",
      "Iteration 1465, loss = 0.50612497\n",
      "Iteration 1466, loss = 0.50598810\n",
      "Iteration 1467, loss = 0.50585127\n",
      "Iteration 1468, loss = 0.50571449\n",
      "Iteration 1469, loss = 0.50557775\n",
      "Iteration 1470, loss = 0.50544105\n",
      "Iteration 1471, loss = 0.50530439\n",
      "Iteration 1472, loss = 0.50516778\n",
      "Iteration 1473, loss = 0.50503121\n",
      "Iteration 1474, loss = 0.50489468\n",
      "Iteration 1475, loss = 0.50475820\n",
      "Iteration 1476, loss = 0.50462176\n",
      "Iteration 1477, loss = 0.50448536\n",
      "Iteration 1478, loss = 0.50434900\n",
      "Iteration 1479, loss = 0.50421269\n",
      "Iteration 1480, loss = 0.50407642\n",
      "Iteration 1481, loss = 0.50394020\n",
      "Iteration 1482, loss = 0.50380401\n",
      "Iteration 1483, loss = 0.50366787\n",
      "Iteration 1484, loss = 0.50353177\n",
      "Iteration 1485, loss = 0.50339572\n",
      "Iteration 1486, loss = 0.50325970\n",
      "Iteration 1487, loss = 0.50312373\n",
      "Iteration 1488, loss = 0.50298781\n",
      "Iteration 1489, loss = 0.50285192\n",
      "Iteration 1490, loss = 0.50271608\n",
      "Iteration 1491, loss = 0.50258028\n",
      "Iteration 1492, loss = 0.50244453\n",
      "Iteration 1493, loss = 0.50230882\n",
      "Iteration 1494, loss = 0.50217315\n",
      "Iteration 1495, loss = 0.50203752\n",
      "Iteration 1496, loss = 0.50190193\n",
      "Iteration 1497, loss = 0.50176639\n",
      "Iteration 1498, loss = 0.50163089\n",
      "Iteration 1499, loss = 0.50149544\n",
      "Iteration 1500, loss = 0.50136002\n",
      "Iteration 1501, loss = 0.50122465\n",
      "Iteration 1502, loss = 0.50108933\n",
      "Iteration 1503, loss = 0.50095404\n",
      "Iteration 1504, loss = 0.50081880\n",
      "Iteration 1505, loss = 0.50068360\n",
      "Iteration 1506, loss = 0.50054845\n",
      "Iteration 1507, loss = 0.50041333\n",
      "Iteration 1508, loss = 0.50027826\n",
      "Iteration 1509, loss = 0.50014323\n",
      "Iteration 1510, loss = 0.50000825\n",
      "Iteration 1511, loss = 0.49987331\n",
      "Iteration 1512, loss = 0.49973841\n",
      "Iteration 1513, loss = 0.49960355\n",
      "Iteration 1514, loss = 0.49946874\n",
      "Iteration 1515, loss = 0.49933396\n",
      "Iteration 1516, loss = 0.49919924\n",
      "Iteration 1517, loss = 0.49906455\n",
      "Iteration 1518, loss = 0.49892991\n",
      "Iteration 1519, loss = 0.49879531\n",
      "Iteration 1520, loss = 0.49866075\n",
      "Iteration 1521, loss = 0.49852624\n",
      "Iteration 1522, loss = 0.49839176\n",
      "Iteration 1523, loss = 0.49825733\n",
      "Iteration 1524, loss = 0.49812295\n",
      "Iteration 1525, loss = 0.49798860\n",
      "Iteration 1526, loss = 0.49785430\n",
      "Iteration 1527, loss = 0.49772004\n",
      "Iteration 1528, loss = 0.49758583\n",
      "Iteration 1529, loss = 0.49745166\n",
      "Iteration 1530, loss = 0.49731753\n",
      "Iteration 1531, loss = 0.49718344\n",
      "Iteration 1532, loss = 0.49704939\n",
      "Iteration 1533, loss = 0.49691539\n",
      "Iteration 1534, loss = 0.49678143\n",
      "Iteration 1535, loss = 0.49664752\n",
      "Iteration 1536, loss = 0.49651364\n",
      "Iteration 1537, loss = 0.49637981\n",
      "Iteration 1538, loss = 0.49624602\n",
      "Iteration 1539, loss = 0.49611228\n",
      "Iteration 1540, loss = 0.49597857\n",
      "Iteration 1541, loss = 0.49584491\n",
      "Iteration 1542, loss = 0.49571130\n",
      "Iteration 1543, loss = 0.49557772\n",
      "Iteration 1544, loss = 0.49544419\n",
      "Iteration 1545, loss = 0.49531070\n",
      "Iteration 1546, loss = 0.49517725\n",
      "Iteration 1547, loss = 0.49504385\n",
      "Iteration 1548, loss = 0.49491049\n",
      "Iteration 1549, loss = 0.49477717\n",
      "Iteration 1550, loss = 0.49464389\n",
      "Iteration 1551, loss = 0.49451066\n",
      "Iteration 1552, loss = 0.49437747\n",
      "Iteration 1553, loss = 0.49424432\n",
      "Iteration 1554, loss = 0.49411121\n",
      "Iteration 1555, loss = 0.49397815\n",
      "Iteration 1556, loss = 0.49384513\n",
      "Iteration 1557, loss = 0.49371215\n",
      "Iteration 1558, loss = 0.49357921\n",
      "Iteration 1559, loss = 0.49344632\n",
      "Iteration 1560, loss = 0.49331347\n",
      "Iteration 1561, loss = 0.49318066\n",
      "Iteration 1562, loss = 0.49304790\n",
      "Iteration 1563, loss = 0.49291518\n",
      "Iteration 1564, loss = 0.49278250\n",
      "Iteration 1565, loss = 0.49264986\n",
      "Iteration 1566, loss = 0.49251727\n",
      "Iteration 1567, loss = 0.49238471\n",
      "Iteration 1568, loss = 0.49225220\n",
      "Iteration 1569, loss = 0.49211974\n",
      "Iteration 1570, loss = 0.49198731\n",
      "Iteration 1571, loss = 0.49185493\n",
      "Iteration 1572, loss = 0.49172259\n",
      "Iteration 1573, loss = 0.49159030\n",
      "Iteration 1574, loss = 0.49145804\n",
      "Iteration 1575, loss = 0.49132583\n",
      "Iteration 1576, loss = 0.49119366\n",
      "Iteration 1577, loss = 0.49106153\n",
      "Iteration 1578, loss = 0.49092945\n",
      "Iteration 1579, loss = 0.49079741\n",
      "Iteration 1580, loss = 0.49066541\n",
      "Iteration 1581, loss = 0.49053345\n",
      "Iteration 1582, loss = 0.49040154\n",
      "Iteration 1583, loss = 0.49026967\n",
      "Iteration 1584, loss = 0.49013784\n",
      "Iteration 1585, loss = 0.49000605\n",
      "Iteration 1586, loss = 0.48987431\n",
      "Iteration 1587, loss = 0.48974261\n",
      "Iteration 1588, loss = 0.48961095\n",
      "Iteration 1589, loss = 0.48947933\n",
      "Iteration 1590, loss = 0.48934776\n",
      "Iteration 1591, loss = 0.48921623\n",
      "Iteration 1592, loss = 0.48908474\n",
      "Iteration 1593, loss = 0.48895329\n",
      "Iteration 1594, loss = 0.48882189\n",
      "Iteration 1595, loss = 0.48869052\n",
      "Iteration 1596, loss = 0.48855921\n",
      "Iteration 1597, loss = 0.48842793\n",
      "Iteration 1598, loss = 0.48829669\n",
      "Iteration 1599, loss = 0.48816550\n",
      "Iteration 1600, loss = 0.48803435\n",
      "Iteration 1601, loss = 0.48790325\n",
      "Iteration 1602, loss = 0.48777218\n",
      "Iteration 1603, loss = 0.48764116\n",
      "Iteration 1604, loss = 0.48751018\n",
      "Iteration 1605, loss = 0.48737924\n",
      "Iteration 1606, loss = 0.48724835\n",
      "Iteration 1607, loss = 0.48711749\n",
      "Iteration 1608, loss = 0.48698668\n",
      "Iteration 1609, loss = 0.48685591\n",
      "Iteration 1610, loss = 0.48672519\n",
      "Iteration 1611, loss = 0.48659451\n",
      "Iteration 1612, loss = 0.48646386\n",
      "Iteration 1613, loss = 0.48633327\n",
      "Iteration 1614, loss = 0.48620271\n",
      "Iteration 1615, loss = 0.48607220\n",
      "Iteration 1616, loss = 0.48594172\n",
      "Iteration 1617, loss = 0.48581129\n",
      "Iteration 1618, loss = 0.48568091\n",
      "Iteration 1619, loss = 0.48555056\n",
      "Iteration 1620, loss = 0.48542026\n",
      "Iteration 1621, loss = 0.48529000\n",
      "Iteration 1622, loss = 0.48515978\n",
      "Iteration 1623, loss = 0.48502961\n",
      "Iteration 1624, loss = 0.48489947\n",
      "Iteration 1625, loss = 0.48476938\n",
      "Iteration 1626, loss = 0.48463933\n",
      "Iteration 1627, loss = 0.48450933\n",
      "Iteration 1628, loss = 0.48437936\n",
      "Iteration 1629, loss = 0.48424944\n",
      "Iteration 1630, loss = 0.48411956\n",
      "Iteration 1631, loss = 0.48398973\n",
      "Iteration 1632, loss = 0.48385993\n",
      "Iteration 1633, loss = 0.48373018\n",
      "Iteration 1634, loss = 0.48360047\n",
      "Iteration 1635, loss = 0.48347080\n",
      "Iteration 1636, loss = 0.48334117\n",
      "Iteration 1637, loss = 0.48321159\n",
      "Iteration 1638, loss = 0.48308205\n",
      "Iteration 1639, loss = 0.48295255\n",
      "Iteration 1640, loss = 0.48282309\n",
      "Iteration 1641, loss = 0.48269367\n",
      "Iteration 1642, loss = 0.48256430\n",
      "Iteration 1643, loss = 0.48243497\n",
      "Iteration 1644, loss = 0.48230568\n",
      "Iteration 1645, loss = 0.48217644\n",
      "Iteration 1646, loss = 0.48204723\n",
      "Iteration 1647, loss = 0.48191807\n",
      "Iteration 1648, loss = 0.48178895\n",
      "Iteration 1649, loss = 0.48165987\n",
      "Iteration 1650, loss = 0.48153084\n",
      "Iteration 1651, loss = 0.48140184\n",
      "Iteration 1652, loss = 0.48127289\n",
      "Iteration 1653, loss = 0.48114398\n",
      "Iteration 1654, loss = 0.48101511\n",
      "Iteration 1655, loss = 0.48088629\n",
      "Iteration 1656, loss = 0.48075750\n",
      "Iteration 1657, loss = 0.48062876\n",
      "Iteration 1658, loss = 0.48050006\n",
      "Iteration 1659, loss = 0.48037141\n",
      "Iteration 1660, loss = 0.48024279\n",
      "Iteration 1661, loss = 0.48011422\n",
      "Iteration 1662, loss = 0.47998569\n",
      "Iteration 1663, loss = 0.47985720\n",
      "Iteration 1664, loss = 0.47972875\n",
      "Iteration 1665, loss = 0.47960035\n",
      "Iteration 1666, loss = 0.47947199\n",
      "Iteration 1667, loss = 0.47934367\n",
      "Iteration 1668, loss = 0.47921539\n",
      "Iteration 1669, loss = 0.47908715\n",
      "Iteration 1670, loss = 0.47895896\n",
      "Iteration 1671, loss = 0.47883080\n",
      "Iteration 1672, loss = 0.47870269\n",
      "Iteration 1673, loss = 0.47857462\n",
      "Iteration 1674, loss = 0.47844660\n",
      "Iteration 1675, loss = 0.47831861\n",
      "Iteration 1676, loss = 0.47819067\n",
      "Iteration 1677, loss = 0.47806277\n",
      "Iteration 1678, loss = 0.47793491\n",
      "Iteration 1679, loss = 0.47780709\n",
      "Iteration 1680, loss = 0.47767932\n",
      "Iteration 1681, loss = 0.47755158\n",
      "Iteration 1682, loss = 0.47742389\n",
      "Iteration 1683, loss = 0.47729624\n",
      "Iteration 1684, loss = 0.47716864\n",
      "Iteration 1685, loss = 0.47704107\n",
      "Iteration 1686, loss = 0.47691355\n",
      "Iteration 1687, loss = 0.47678606\n",
      "Iteration 1688, loss = 0.47665862\n",
      "Iteration 1689, loss = 0.47653123\n",
      "Iteration 1690, loss = 0.47640387\n",
      "Iteration 1691, loss = 0.47627656\n",
      "Iteration 1692, loss = 0.47614928\n",
      "Iteration 1693, loss = 0.47602205\n",
      "Iteration 1694, loss = 0.47589486\n",
      "Iteration 1695, loss = 0.47576772\n",
      "Iteration 1696, loss = 0.47564061\n",
      "Iteration 1697, loss = 0.47551355\n",
      "Iteration 1698, loss = 0.47538653\n",
      "Iteration 1699, loss = 0.47525955\n",
      "Iteration 1700, loss = 0.47513261\n",
      "Iteration 1701, loss = 0.47500571\n",
      "Iteration 1702, loss = 0.47487886\n",
      "Iteration 1703, loss = 0.47475204\n",
      "Iteration 1704, loss = 0.47462527\n",
      "Iteration 1705, loss = 0.47449854\n",
      "Iteration 1706, loss = 0.47437186\n",
      "Iteration 1707, loss = 0.47424521\n",
      "Iteration 1708, loss = 0.47411861\n",
      "Iteration 1709, loss = 0.47399204\n",
      "Iteration 1710, loss = 0.47386552\n",
      "Iteration 1711, loss = 0.47373904\n",
      "Iteration 1712, loss = 0.47361261\n",
      "Iteration 1713, loss = 0.47348621\n",
      "Iteration 1714, loss = 0.47335986\n",
      "Iteration 1715, loss = 0.47323354\n",
      "Iteration 1716, loss = 0.47310727\n",
      "Iteration 1717, loss = 0.47298104\n",
      "Iteration 1718, loss = 0.47285486\n",
      "Iteration 1719, loss = 0.47272871\n",
      "Iteration 1720, loss = 0.47260261\n",
      "Iteration 1721, loss = 0.47247654\n",
      "Iteration 1722, loss = 0.47235052\n",
      "Iteration 1723, loss = 0.47222454\n",
      "Iteration 1724, loss = 0.47209861\n",
      "Iteration 1725, loss = 0.47197271\n",
      "Iteration 1726, loss = 0.47184685\n",
      "Iteration 1727, loss = 0.47172104\n",
      "Iteration 1728, loss = 0.47159527\n",
      "Iteration 1729, loss = 0.47146954\n",
      "Iteration 1730, loss = 0.47134385\n",
      "Iteration 1731, loss = 0.47121820\n",
      "Iteration 1732, loss = 0.47109260\n",
      "Iteration 1733, loss = 0.47096704\n",
      "Iteration 1734, loss = 0.47084151\n",
      "Iteration 1735, loss = 0.47071603\n",
      "Iteration 1736, loss = 0.47059059\n",
      "Iteration 1737, loss = 0.47046520\n",
      "Iteration 1738, loss = 0.47033984\n",
      "Iteration 1739, loss = 0.47021452\n",
      "Iteration 1740, loss = 0.47008925\n",
      "Iteration 1741, loss = 0.46996402\n",
      "Iteration 1742, loss = 0.46983883\n",
      "Iteration 1743, loss = 0.46971368\n",
      "Iteration 1744, loss = 0.46958857\n",
      "Iteration 1745, loss = 0.46946350\n",
      "Iteration 1746, loss = 0.46933848\n",
      "Iteration 1747, loss = 0.46921350\n",
      "Iteration 1748, loss = 0.46908855\n",
      "Iteration 1749, loss = 0.46896365\n",
      "Iteration 1750, loss = 0.46883879\n",
      "Iteration 1751, loss = 0.46871398\n",
      "Iteration 1752, loss = 0.46858920\n",
      "Iteration 1753, loss = 0.46846446\n",
      "Iteration 1754, loss = 0.46833977\n",
      "Iteration 1755, loss = 0.46821512\n",
      "Iteration 1756, loss = 0.46809051\n",
      "Iteration 1757, loss = 0.46796594\n",
      "Iteration 1758, loss = 0.46784141\n",
      "Iteration 1759, loss = 0.46771692\n",
      "Iteration 1760, loss = 0.46759247\n",
      "Iteration 1761, loss = 0.46746807\n",
      "Iteration 1762, loss = 0.46734370\n",
      "Iteration 1763, loss = 0.46721938\n",
      "Iteration 1764, loss = 0.46709510\n",
      "Iteration 1765, loss = 0.46697086\n",
      "Iteration 1766, loss = 0.46684666\n",
      "Iteration 1767, loss = 0.46672251\n",
      "Iteration 1768, loss = 0.46659839\n",
      "Iteration 1769, loss = 0.46647431\n",
      "Iteration 1770, loss = 0.46635028\n",
      "Iteration 1771, loss = 0.46622629\n",
      "Iteration 1772, loss = 0.46610234\n",
      "Iteration 1773, loss = 0.46597843\n",
      "Iteration 1774, loss = 0.46585456\n",
      "Iteration 1775, loss = 0.46573073\n",
      "Iteration 1776, loss = 0.46560694\n",
      "Iteration 1777, loss = 0.46548320\n",
      "Iteration 1778, loss = 0.46535949\n",
      "Iteration 1779, loss = 0.46523583\n",
      "Iteration 1780, loss = 0.46511221\n",
      "Iteration 1781, loss = 0.46498863\n",
      "Iteration 1782, loss = 0.46486509\n",
      "Iteration 1783, loss = 0.46474159\n",
      "Iteration 1784, loss = 0.46461813\n",
      "Iteration 1785, loss = 0.46449471\n",
      "Iteration 1786, loss = 0.46437134\n",
      "Iteration 1787, loss = 0.46424800\n",
      "Iteration 1788, loss = 0.46412471\n",
      "Iteration 1789, loss = 0.46400145\n",
      "Iteration 1790, loss = 0.46387824\n",
      "Iteration 1791, loss = 0.46375507\n",
      "Iteration 1792, loss = 0.46363194\n",
      "Iteration 1793, loss = 0.46350885\n",
      "Iteration 1794, loss = 0.46338581\n",
      "Iteration 1795, loss = 0.46326280\n",
      "Iteration 1796, loss = 0.46313983\n",
      "Iteration 1797, loss = 0.46301691\n",
      "Iteration 1798, loss = 0.46289402\n",
      "Iteration 1799, loss = 0.46277118\n",
      "Iteration 1800, loss = 0.46264838\n",
      "Iteration 1801, loss = 0.46252562\n",
      "Iteration 1802, loss = 0.46240290\n",
      "Iteration 1803, loss = 0.46228022\n",
      "Iteration 1804, loss = 0.46215758\n",
      "Iteration 1805, loss = 0.46203498\n",
      "Iteration 1806, loss = 0.46191242\n",
      "Iteration 1807, loss = 0.46178991\n",
      "Iteration 1808, loss = 0.46166743\n",
      "Iteration 1809, loss = 0.46154500\n",
      "Iteration 1810, loss = 0.46142261\n",
      "Iteration 1811, loss = 0.46130025\n",
      "Iteration 1812, loss = 0.46117794\n",
      "Iteration 1813, loss = 0.46105567\n",
      "Iteration 1814, loss = 0.46093344\n",
      "Iteration 1815, loss = 0.46081125\n",
      "Iteration 1816, loss = 0.46068910\n",
      "Iteration 1817, loss = 0.46056699\n",
      "Iteration 1818, loss = 0.46044493\n",
      "Iteration 1819, loss = 0.46032290\n",
      "Iteration 1820, loss = 0.46020091\n",
      "Iteration 1821, loss = 0.46007897\n",
      "Iteration 1822, loss = 0.45995706\n",
      "Iteration 1823, loss = 0.45983520\n",
      "Iteration 1824, loss = 0.45971338\n",
      "Iteration 1825, loss = 0.45959159\n",
      "Iteration 1826, loss = 0.45946985\n",
      "Iteration 1827, loss = 0.45934815\n",
      "Iteration 1828, loss = 0.45922649\n",
      "Iteration 1829, loss = 0.45910487\n",
      "Iteration 1830, loss = 0.45898329\n",
      "Iteration 1831, loss = 0.45886175\n",
      "Iteration 1832, loss = 0.45874026\n",
      "Iteration 1833, loss = 0.45861880\n",
      "Iteration 1834, loss = 0.45849738\n",
      "Iteration 1835, loss = 0.45837601\n",
      "Iteration 1836, loss = 0.45825467\n",
      "Iteration 1837, loss = 0.45813338\n",
      "Iteration 1838, loss = 0.45801212\n",
      "Iteration 1839, loss = 0.45789091\n",
      "Iteration 1840, loss = 0.45776973\n",
      "Iteration 1841, loss = 0.45764860\n",
      "Iteration 1842, loss = 0.45752751\n",
      "Iteration 1843, loss = 0.45740646\n",
      "Iteration 1844, loss = 0.45728545\n",
      "Iteration 1845, loss = 0.45716447\n",
      "Iteration 1846, loss = 0.45704354\n",
      "Iteration 1847, loss = 0.45692265\n",
      "Iteration 1848, loss = 0.45680180\n",
      "Iteration 1849, loss = 0.45668099\n",
      "Iteration 1850, loss = 0.45656023\n",
      "Iteration 1851, loss = 0.45643950\n",
      "Iteration 1852, loss = 0.45631881\n",
      "Iteration 1853, loss = 0.45619816\n",
      "Iteration 1854, loss = 0.45607755\n",
      "Iteration 1855, loss = 0.45595699\n",
      "Iteration 1856, loss = 0.45583646\n",
      "Iteration 1857, loss = 0.45571598\n",
      "Iteration 1858, loss = 0.45559553\n",
      "Iteration 1859, loss = 0.45547512\n",
      "Iteration 1860, loss = 0.45535476\n",
      "Iteration 1861, loss = 0.45523443\n",
      "Iteration 1862, loss = 0.45511415\n",
      "Iteration 1863, loss = 0.45499391\n",
      "Iteration 1864, loss = 0.45487370\n",
      "Iteration 1865, loss = 0.45475354\n",
      "Iteration 1866, loss = 0.45463341\n",
      "Iteration 1867, loss = 0.45451333\n",
      "Iteration 1868, loss = 0.45439329\n",
      "Iteration 1869, loss = 0.45427328\n",
      "Iteration 1870, loss = 0.45415332\n",
      "Iteration 1871, loss = 0.45403340\n",
      "Iteration 1872, loss = 0.45391352\n",
      "Iteration 1873, loss = 0.45379368\n",
      "Iteration 1874, loss = 0.45367387\n",
      "Iteration 1875, loss = 0.45355411\n",
      "Iteration 1876, loss = 0.45343439\n",
      "Iteration 1877, loss = 0.45331471\n",
      "Iteration 1878, loss = 0.45319507\n",
      "Iteration 1879, loss = 0.45307547\n",
      "Iteration 1880, loss = 0.45295591\n",
      "Iteration 1881, loss = 0.45283638\n",
      "Iteration 1882, loss = 0.45271690\n",
      "Iteration 1883, loss = 0.45259746\n",
      "Iteration 1884, loss = 0.45247806\n",
      "Iteration 1885, loss = 0.45235870\n",
      "Iteration 1886, loss = 0.45223938\n",
      "Iteration 1887, loss = 0.45212010\n",
      "Iteration 1888, loss = 0.45200086\n",
      "Iteration 1889, loss = 0.45188166\n",
      "Iteration 1890, loss = 0.45176250\n",
      "Iteration 1891, loss = 0.45164338\n",
      "Iteration 1892, loss = 0.45152430\n",
      "Iteration 1893, loss = 0.45140525\n",
      "Iteration 1894, loss = 0.45128625\n",
      "Iteration 1895, loss = 0.45116729\n",
      "Iteration 1896, loss = 0.45104837\n",
      "Iteration 1897, loss = 0.45092949\n",
      "Iteration 1898, loss = 0.45081065\n",
      "Iteration 1899, loss = 0.45069185\n",
      "Iteration 1900, loss = 0.45057309\n",
      "Iteration 1901, loss = 0.45045437\n",
      "Iteration 1902, loss = 0.45033568\n",
      "Iteration 1903, loss = 0.45021704\n",
      "Iteration 1904, loss = 0.45009844\n",
      "Iteration 1905, loss = 0.44997988\n",
      "Iteration 1906, loss = 0.44986136\n",
      "Iteration 1907, loss = 0.44974287\n",
      "Iteration 1908, loss = 0.44962443\n",
      "Iteration 1909, loss = 0.44950603\n",
      "Iteration 1910, loss = 0.44938766\n",
      "Iteration 1911, loss = 0.44926934\n",
      "Iteration 1912, loss = 0.44915106\n",
      "Iteration 1913, loss = 0.44903281\n",
      "Iteration 1914, loss = 0.44891461\n",
      "Iteration 1915, loss = 0.44879644\n",
      "Iteration 1916, loss = 0.44867832\n",
      "Iteration 1917, loss = 0.44856023\n",
      "Iteration 1918, loss = 0.44844219\n",
      "Iteration 1919, loss = 0.44832418\n",
      "Iteration 1920, loss = 0.44820622\n",
      "Iteration 1921, loss = 0.44808829\n",
      "Iteration 1922, loss = 0.44797040\n",
      "Iteration 1923, loss = 0.44785256\n",
      "Iteration 1924, loss = 0.44773475\n",
      "Iteration 1925, loss = 0.44761698\n",
      "Iteration 1926, loss = 0.44749925\n",
      "Iteration 1927, loss = 0.44738156\n",
      "Iteration 1928, loss = 0.44726391\n",
      "Iteration 1929, loss = 0.44714630\n",
      "Iteration 1930, loss = 0.44702873\n",
      "Iteration 1931, loss = 0.44691120\n",
      "Iteration 1932, loss = 0.44679371\n",
      "Iteration 1933, loss = 0.44667626\n",
      "Iteration 1934, loss = 0.44655885\n",
      "Iteration 1935, loss = 0.44644148\n",
      "Iteration 1936, loss = 0.44632414\n",
      "Iteration 1937, loss = 0.44620685\n",
      "Iteration 1938, loss = 0.44608959\n",
      "Iteration 1939, loss = 0.44597238\n",
      "Iteration 1940, loss = 0.44585520\n",
      "Iteration 1941, loss = 0.44573807\n",
      "Iteration 1942, loss = 0.44562097\n",
      "Iteration 1943, loss = 0.44550391\n",
      "Iteration 1944, loss = 0.44538690\n",
      "Iteration 1945, loss = 0.44526992\n",
      "Iteration 1946, loss = 0.44515298\n",
      "Iteration 1947, loss = 0.44503608\n",
      "Iteration 1948, loss = 0.44491922\n",
      "Iteration 1949, loss = 0.44480240\n",
      "Iteration 1950, loss = 0.44468562\n",
      "Iteration 1951, loss = 0.44456887\n",
      "Iteration 1952, loss = 0.44445217\n",
      "Iteration 1953, loss = 0.44433551\n",
      "Iteration 1954, loss = 0.44421888\n",
      "Iteration 1955, loss = 0.44410230\n",
      "Iteration 1956, loss = 0.44398575\n",
      "Iteration 1957, loss = 0.44386924\n",
      "Iteration 1958, loss = 0.44375277\n",
      "Iteration 1959, loss = 0.44363635\n",
      "Iteration 1960, loss = 0.44351996\n",
      "Iteration 1961, loss = 0.44340361\n",
      "Iteration 1962, loss = 0.44328729\n",
      "Iteration 1963, loss = 0.44317102\n",
      "Iteration 1964, loss = 0.44305479\n",
      "Iteration 1965, loss = 0.44293860\n",
      "Iteration 1966, loss = 0.44282244\n",
      "Iteration 1967, loss = 0.44270633\n",
      "Iteration 1968, loss = 0.44259025\n",
      "Iteration 1969, loss = 0.44247421\n",
      "Iteration 1970, loss = 0.44235821\n",
      "Iteration 1971, loss = 0.44224226\n",
      "Iteration 1972, loss = 0.44212634\n",
      "Iteration 1973, loss = 0.44201045\n",
      "Iteration 1974, loss = 0.44189461\n",
      "Iteration 1975, loss = 0.44177881\n",
      "Iteration 1976, loss = 0.44166305\n",
      "Iteration 1977, loss = 0.44154732\n",
      "Iteration 1978, loss = 0.44143163\n",
      "Iteration 1979, loss = 0.44131599\n",
      "Iteration 1980, loss = 0.44120038\n",
      "Iteration 1981, loss = 0.44108481\n",
      "Iteration 1982, loss = 0.44096928\n",
      "Iteration 1983, loss = 0.44085379\n",
      "Iteration 1984, loss = 0.44073834\n",
      "Iteration 1985, loss = 0.44062292\n",
      "Iteration 1986, loss = 0.44050755\n",
      "Iteration 1987, loss = 0.44039221\n",
      "Iteration 1988, loss = 0.44027691\n",
      "Iteration 1989, loss = 0.44016166\n",
      "Iteration 1990, loss = 0.44004644\n",
      "Iteration 1991, loss = 0.43993126\n",
      "Iteration 1992, loss = 0.43981611\n",
      "Iteration 1993, loss = 0.43970101\n",
      "Iteration 1994, loss = 0.43958595\n",
      "Iteration 1995, loss = 0.43947092\n",
      "Iteration 1996, loss = 0.43935594\n",
      "Iteration 1997, loss = 0.43924099\n",
      "Iteration 1998, loss = 0.43912608\n",
      "Iteration 1999, loss = 0.43901121\n",
      "Iteration 2000, loss = 0.43889638\n",
      "Iteration 2001, loss = 0.43878158\n",
      "Iteration 2002, loss = 0.43866683\n",
      "Iteration 2003, loss = 0.43855211\n",
      "Iteration 2004, loss = 0.43843744\n",
      "Iteration 2005, loss = 0.43832280\n",
      "Iteration 2006, loss = 0.43820820\n",
      "Iteration 2007, loss = 0.43809364\n",
      "Iteration 2008, loss = 0.43797911\n",
      "Iteration 2009, loss = 0.43786463\n",
      "Iteration 2010, loss = 0.43775018\n",
      "Iteration 2011, loss = 0.43763578\n",
      "Iteration 2012, loss = 0.43752141\n",
      "Iteration 2013, loss = 0.43740708\n",
      "Iteration 2014, loss = 0.43729279\n",
      "Iteration 2015, loss = 0.43717854\n",
      "Iteration 2016, loss = 0.43706432\n",
      "Iteration 2017, loss = 0.43695015\n",
      "Iteration 2018, loss = 0.43683601\n",
      "Iteration 2019, loss = 0.43672191\n",
      "Iteration 2020, loss = 0.43660785\n",
      "Iteration 2021, loss = 0.43649383\n",
      "Iteration 2022, loss = 0.43637984\n",
      "Iteration 2023, loss = 0.43626590\n",
      "Iteration 2024, loss = 0.43615199\n",
      "Iteration 2025, loss = 0.43603812\n",
      "Iteration 2026, loss = 0.43592429\n",
      "Iteration 2027, loss = 0.43581050\n",
      "Iteration 2028, loss = 0.43569675\n",
      "Iteration 2029, loss = 0.43558304\n",
      "Iteration 2030, loss = 0.43546936\n",
      "Iteration 2031, loss = 0.43535572\n",
      "Iteration 2032, loss = 0.43524212\n",
      "Iteration 2033, loss = 0.43512856\n",
      "Iteration 2034, loss = 0.43501504\n",
      "Iteration 2035, loss = 0.43490155\n",
      "Iteration 2036, loss = 0.43478810\n",
      "Iteration 2037, loss = 0.43467470\n",
      "Iteration 2038, loss = 0.43456133\n",
      "Iteration 2039, loss = 0.43444799\n",
      "Iteration 2040, loss = 0.43433470\n",
      "Iteration 2041, loss = 0.43422144\n",
      "Iteration 2042, loss = 0.43410823\n",
      "Iteration 2043, loss = 0.43399505\n",
      "Iteration 2044, loss = 0.43388191\n",
      "Iteration 2045, loss = 0.43376880\n",
      "Iteration 2046, loss = 0.43365574\n",
      "Iteration 2047, loss = 0.43354271\n",
      "Iteration 2048, loss = 0.43342972\n",
      "Iteration 2049, loss = 0.43331677\n",
      "Iteration 2050, loss = 0.43320386\n",
      "Iteration 2051, loss = 0.43309099\n",
      "Iteration 2052, loss = 0.43297815\n",
      "Iteration 2053, loss = 0.43286535\n",
      "Iteration 2054, loss = 0.43275259\n",
      "Iteration 2055, loss = 0.43263987\n",
      "Iteration 2056, loss = 0.43252718\n",
      "Iteration 2057, loss = 0.43241454\n",
      "Iteration 2058, loss = 0.43230193\n",
      "Iteration 2059, loss = 0.43218936\n",
      "Iteration 2060, loss = 0.43207683\n",
      "Iteration 2061, loss = 0.43196433\n",
      "Iteration 2062, loss = 0.43185188\n",
      "Iteration 2063, loss = 0.43173946\n",
      "Iteration 2064, loss = 0.43162708\n",
      "Iteration 2065, loss = 0.43151473\n",
      "Iteration 2066, loss = 0.43140243\n",
      "Iteration 2067, loss = 0.43129016\n",
      "Iteration 2068, loss = 0.43117793\n",
      "Iteration 2069, loss = 0.43106574\n",
      "Iteration 2070, loss = 0.43095359\n",
      "Iteration 2071, loss = 0.43084147\n",
      "Iteration 2072, loss = 0.43072939\n",
      "Iteration 2073, loss = 0.43061735\n",
      "Iteration 2074, loss = 0.43050535\n",
      "Iteration 2075, loss = 0.43039338\n",
      "Iteration 2076, loss = 0.43028146\n",
      "Iteration 2077, loss = 0.43016957\n",
      "Iteration 2078, loss = 0.43005772\n",
      "Iteration 2079, loss = 0.42994590\n",
      "Iteration 2080, loss = 0.42983413\n",
      "Iteration 2081, loss = 0.42972239\n",
      "Iteration 2082, loss = 0.42961069\n",
      "Iteration 2083, loss = 0.42949902\n",
      "Iteration 2084, loss = 0.42938740\n",
      "Iteration 2085, loss = 0.42927581\n",
      "Iteration 2086, loss = 0.42916426\n",
      "Iteration 2087, loss = 0.42905275\n",
      "Iteration 2088, loss = 0.42894127\n",
      "Iteration 2089, loss = 0.42882983\n",
      "Iteration 2090, loss = 0.42871843\n",
      "Iteration 2091, loss = 0.42860707\n",
      "Iteration 2092, loss = 0.42849575\n",
      "Iteration 2093, loss = 0.42838446\n",
      "Iteration 2094, loss = 0.42827321\n",
      "Iteration 2095, loss = 0.42816200\n",
      "Iteration 2096, loss = 0.42805082\n",
      "Iteration 2097, loss = 0.42793968\n",
      "Iteration 2098, loss = 0.42782858\n",
      "Iteration 2099, loss = 0.42771752\n",
      "Iteration 2100, loss = 0.42760650\n",
      "Iteration 2101, loss = 0.42749551\n",
      "Iteration 2102, loss = 0.42738456\n",
      "Iteration 2103, loss = 0.42727364\n",
      "Iteration 2104, loss = 0.42716277\n",
      "Iteration 2105, loss = 0.42705193\n",
      "Iteration 2106, loss = 0.42694113\n",
      "Iteration 2107, loss = 0.42683037\n",
      "Iteration 2108, loss = 0.42671964\n",
      "Iteration 2109, loss = 0.42660895\n",
      "Iteration 2110, loss = 0.42649830\n",
      "Iteration 2111, loss = 0.42638768\n",
      "Iteration 2112, loss = 0.42627711\n",
      "Iteration 2113, loss = 0.42616657\n",
      "Iteration 2114, loss = 0.42605606\n",
      "Iteration 2115, loss = 0.42594560\n",
      "Iteration 2116, loss = 0.42583517\n",
      "Iteration 2117, loss = 0.42572478\n",
      "Iteration 2118, loss = 0.42561443\n",
      "Iteration 2119, loss = 0.42550411\n",
      "Iteration 2120, loss = 0.42539383\n",
      "Iteration 2121, loss = 0.42528359\n",
      "Iteration 2122, loss = 0.42517338\n",
      "Iteration 2123, loss = 0.42506321\n",
      "Iteration 2124, loss = 0.42495308\n",
      "Iteration 2125, loss = 0.42484299\n",
      "Iteration 2126, loss = 0.42473293\n",
      "Iteration 2127, loss = 0.42462291\n",
      "Iteration 2128, loss = 0.42451293\n",
      "Iteration 2129, loss = 0.42440298\n",
      "Iteration 2130, loss = 0.42429307\n",
      "Iteration 2131, loss = 0.42418320\n",
      "Iteration 2132, loss = 0.42407337\n",
      "Iteration 2133, loss = 0.42396357\n",
      "Iteration 2134, loss = 0.42385381\n",
      "Iteration 2135, loss = 0.42374409\n",
      "Iteration 2136, loss = 0.42363440\n",
      "Iteration 2137, loss = 0.42352475\n",
      "Iteration 2138, loss = 0.42341514\n",
      "Iteration 2139, loss = 0.42330556\n",
      "Iteration 2140, loss = 0.42319602\n",
      "Iteration 2141, loss = 0.42308652\n",
      "Iteration 2142, loss = 0.42297705\n",
      "Iteration 2143, loss = 0.42286763\n",
      "Iteration 2144, loss = 0.42275823\n",
      "Iteration 2145, loss = 0.42264888\n",
      "Iteration 2146, loss = 0.42253956\n",
      "Iteration 2147, loss = 0.42243028\n",
      "Iteration 2148, loss = 0.42232103\n",
      "Iteration 2149, loss = 0.42221183\n",
      "Iteration 2150, loss = 0.42210266\n",
      "Iteration 2151, loss = 0.42199352\n",
      "Iteration 2152, loss = 0.42188442\n",
      "Iteration 2153, loss = 0.42177536\n",
      "Iteration 2154, loss = 0.42166634\n",
      "Iteration 2155, loss = 0.42155735\n",
      "Iteration 2156, loss = 0.42144840\n",
      "Iteration 2157, loss = 0.42133949\n",
      "Iteration 2158, loss = 0.42123061\n",
      "Iteration 2159, loss = 0.42112177\n",
      "Iteration 2160, loss = 0.42101297\n",
      "Iteration 2161, loss = 0.42090420\n",
      "Iteration 2162, loss = 0.42079547\n",
      "Iteration 2163, loss = 0.42068677\n",
      "Iteration 2164, loss = 0.42057811\n",
      "Iteration 2165, loss = 0.42046949\n",
      "Iteration 2166, loss = 0.42036091\n",
      "Iteration 2167, loss = 0.42025236\n",
      "Iteration 2168, loss = 0.42014385\n",
      "Iteration 2169, loss = 0.42003537\n",
      "Iteration 2170, loss = 0.41992694\n",
      "Iteration 2171, loss = 0.41981853\n",
      "Iteration 2172, loss = 0.41971017\n",
      "Iteration 2173, loss = 0.41960184\n",
      "Iteration 2174, loss = 0.41949355\n",
      "Iteration 2175, loss = 0.41938529\n",
      "Iteration 2176, loss = 0.41927707\n",
      "Iteration 2177, loss = 0.41916889\n",
      "Iteration 2178, loss = 0.41906074\n",
      "Iteration 2179, loss = 0.41895263\n",
      "Iteration 2180, loss = 0.41884455\n",
      "Iteration 2181, loss = 0.41873651\n",
      "Iteration 2182, loss = 0.41862851\n",
      "Iteration 2183, loss = 0.41852055\n",
      "Iteration 2184, loss = 0.41841262\n",
      "Iteration 2185, loss = 0.41830472\n",
      "Iteration 2186, loss = 0.41819687\n",
      "Iteration 2187, loss = 0.41808905\n",
      "Iteration 2188, loss = 0.41798126\n",
      "Iteration 2189, loss = 0.41787352\n",
      "Iteration 2190, loss = 0.41776580\n",
      "Iteration 2191, loss = 0.41765813\n",
      "Iteration 2192, loss = 0.41755049\n",
      "Iteration 2193, loss = 0.41744289\n",
      "Iteration 2194, loss = 0.41733532\n",
      "Iteration 2195, loss = 0.41722779\n",
      "Iteration 2196, loss = 0.41712029\n",
      "Iteration 2197, loss = 0.41701283\n",
      "Iteration 2198, loss = 0.41690541\n",
      "Iteration 2199, loss = 0.41679803\n",
      "Iteration 2200, loss = 0.41669067\n",
      "Iteration 2201, loss = 0.41658336\n",
      "Iteration 2202, loss = 0.41647608\n",
      "Iteration 2203, loss = 0.41636884\n",
      "Iteration 2204, loss = 0.41626163\n",
      "Iteration 2205, loss = 0.41615446\n",
      "Iteration 2206, loss = 0.41604733\n",
      "Iteration 2207, loss = 0.41594023\n",
      "Iteration 2208, loss = 0.41583317\n",
      "Iteration 2209, loss = 0.41572614\n",
      "Iteration 2210, loss = 0.41561915\n",
      "Iteration 2211, loss = 0.41551220\n",
      "Iteration 2212, loss = 0.41540528\n",
      "Iteration 2213, loss = 0.41529840\n",
      "Iteration 2214, loss = 0.41519155\n",
      "Iteration 2215, loss = 0.41508474\n",
      "Iteration 2216, loss = 0.41497796\n",
      "Iteration 2217, loss = 0.41487123\n",
      "Iteration 2218, loss = 0.41476452\n",
      "Iteration 2219, loss = 0.41465785\n",
      "Iteration 2220, loss = 0.41455122\n",
      "Iteration 2221, loss = 0.41444463\n",
      "Iteration 2222, loss = 0.41433807\n",
      "Iteration 2223, loss = 0.41423154\n",
      "Iteration 2224, loss = 0.41412505\n",
      "Iteration 2225, loss = 0.41401860\n",
      "Iteration 2226, loss = 0.41391218\n",
      "Iteration 2227, loss = 0.41380580\n",
      "Iteration 2228, loss = 0.41369946\n",
      "Iteration 2229, loss = 0.41359315\n",
      "Iteration 2230, loss = 0.41348687\n",
      "Iteration 2231, loss = 0.41338063\n",
      "Iteration 2232, loss = 0.41327443\n",
      "Iteration 2233, loss = 0.41316826\n",
      "Iteration 2234, loss = 0.41306213\n",
      "Iteration 2235, loss = 0.41295603\n",
      "Iteration 2236, loss = 0.41284997\n",
      "Iteration 2237, loss = 0.41274395\n",
      "Iteration 2238, loss = 0.41263796\n",
      "Iteration 2239, loss = 0.41253200\n",
      "Iteration 2240, loss = 0.41242609\n",
      "Iteration 2241, loss = 0.41232020\n",
      "Iteration 2242, loss = 0.41221436\n",
      "Iteration 2243, loss = 0.41210854\n",
      "Iteration 2244, loss = 0.41200277\n",
      "Iteration 2245, loss = 0.41189703\n",
      "Iteration 2246, loss = 0.41179132\n",
      "Iteration 2247, loss = 0.41168565\n",
      "Iteration 2248, loss = 0.41158001\n",
      "Iteration 2249, loss = 0.41147441\n",
      "Iteration 2250, loss = 0.41136885\n",
      "Iteration 2251, loss = 0.41126332\n",
      "Iteration 2252, loss = 0.41115783\n",
      "Iteration 2253, loss = 0.41105237\n",
      "Iteration 2254, loss = 0.41094695\n",
      "Iteration 2255, loss = 0.41084156\n",
      "Iteration 2256, loss = 0.41073621\n",
      "Iteration 2257, loss = 0.41063089\n",
      "Iteration 2258, loss = 0.41052561\n",
      "Iteration 2259, loss = 0.41042036\n",
      "Iteration 2260, loss = 0.41031515\n",
      "Iteration 2261, loss = 0.41020998\n",
      "Iteration 2262, loss = 0.41010484\n",
      "Iteration 2263, loss = 0.40999973\n",
      "Iteration 2264, loss = 0.40989466\n",
      "Iteration 2265, loss = 0.40978963\n",
      "Iteration 2266, loss = 0.40968463\n",
      "Iteration 2267, loss = 0.40957966\n",
      "Iteration 2268, loss = 0.40947473\n",
      "Iteration 2269, loss = 0.40936984\n",
      "Iteration 2270, loss = 0.40926498\n",
      "Iteration 2271, loss = 0.40916016\n",
      "Iteration 2272, loss = 0.40905537\n",
      "Iteration 2273, loss = 0.40895061\n",
      "Iteration 2274, loss = 0.40884589\n",
      "Iteration 2275, loss = 0.40874121\n",
      "Iteration 2276, loss = 0.40863656\n",
      "Iteration 2277, loss = 0.40853195\n",
      "Iteration 2278, loss = 0.40842737\n",
      "Iteration 2279, loss = 0.40832282\n",
      "Iteration 2280, loss = 0.40821831\n",
      "Iteration 2281, loss = 0.40811384\n",
      "Iteration 2282, loss = 0.40800940\n",
      "Iteration 2283, loss = 0.40790500\n",
      "Iteration 2284, loss = 0.40780063\n",
      "Iteration 2285, loss = 0.40769629\n",
      "Iteration 2286, loss = 0.40759200\n",
      "Iteration 2287, loss = 0.40748773\n",
      "Iteration 2288, loss = 0.40738350\n",
      "Iteration 2289, loss = 0.40727931\n",
      "Iteration 2290, loss = 0.40717515\n",
      "Iteration 2291, loss = 0.40707102\n",
      "Iteration 2292, loss = 0.40696693\n",
      "Iteration 2293, loss = 0.40686287\n",
      "Iteration 2294, loss = 0.40675885\n",
      "Iteration 2295, loss = 0.40665487\n",
      "Iteration 2296, loss = 0.40655092\n",
      "Iteration 2297, loss = 0.40644700\n",
      "Iteration 2298, loss = 0.40634312\n",
      "Iteration 2299, loss = 0.40623927\n",
      "Iteration 2300, loss = 0.40613546\n",
      "Iteration 2301, loss = 0.40603168\n",
      "Iteration 2302, loss = 0.40592794\n",
      "Iteration 2303, loss = 0.40582423\n",
      "Iteration 2304, loss = 0.40572056\n",
      "Iteration 2305, loss = 0.40561692\n",
      "Iteration 2306, loss = 0.40551331\n",
      "Iteration 2307, loss = 0.40540974\n",
      "Iteration 2308, loss = 0.40530621\n",
      "Iteration 2309, loss = 0.40520271\n",
      "Iteration 2310, loss = 0.40509924\n",
      "Iteration 2311, loss = 0.40499581\n",
      "Iteration 2312, loss = 0.40489241\n",
      "Iteration 2313, loss = 0.40478905\n",
      "Iteration 2314, loss = 0.40468572\n",
      "Iteration 2315, loss = 0.40458243\n",
      "Iteration 2316, loss = 0.40447917\n",
      "Iteration 2317, loss = 0.40437594\n",
      "Iteration 2318, loss = 0.40427275\n",
      "Iteration 2319, loss = 0.40416959\n",
      "Iteration 2320, loss = 0.40406647\n",
      "Iteration 2321, loss = 0.40396339\n",
      "Iteration 2322, loss = 0.40386033\n",
      "Iteration 2323, loss = 0.40375731\n",
      "Iteration 2324, loss = 0.40365433\n",
      "Iteration 2325, loss = 0.40355138\n",
      "Iteration 2326, loss = 0.40344847\n",
      "Iteration 2327, loss = 0.40334558\n",
      "Iteration 2328, loss = 0.40324274\n",
      "Iteration 2329, loss = 0.40313992\n",
      "Iteration 2330, loss = 0.40303715\n",
      "Iteration 2331, loss = 0.40293440\n",
      "Iteration 2332, loss = 0.40283169\n",
      "Iteration 2333, loss = 0.40272902\n",
      "Iteration 2334, loss = 0.40262638\n",
      "Iteration 2335, loss = 0.40252377\n",
      "Iteration 2336, loss = 0.40242120\n",
      "Iteration 2337, loss = 0.40231866\n",
      "Iteration 2338, loss = 0.40221615\n",
      "Iteration 2339, loss = 0.40211368\n",
      "Iteration 2340, loss = 0.40201125\n",
      "Iteration 2341, loss = 0.40190884\n",
      "Iteration 2342, loss = 0.40180648\n",
      "Iteration 2343, loss = 0.40170414\n",
      "Iteration 2344, loss = 0.40160184\n",
      "Iteration 2345, loss = 0.40149958\n",
      "Iteration 2346, loss = 0.40139734\n",
      "Iteration 2347, loss = 0.40129515\n",
      "Iteration 2348, loss = 0.40119298\n",
      "Iteration 2349, loss = 0.40109085\n",
      "Iteration 2350, loss = 0.40098876\n",
      "Iteration 2351, loss = 0.40088669\n",
      "Iteration 2352, loss = 0.40078467\n",
      "Iteration 2353, loss = 0.40068267\n",
      "Iteration 2354, loss = 0.40058071\n",
      "Iteration 2355, loss = 0.40047879\n",
      "Iteration 2356, loss = 0.40037689\n",
      "Iteration 2357, loss = 0.40027504\n",
      "Iteration 2358, loss = 0.40017321\n",
      "Iteration 2359, loss = 0.40007142\n",
      "Iteration 2360, loss = 0.39996966\n",
      "Iteration 2361, loss = 0.39986794\n",
      "Iteration 2362, loss = 0.39976625\n",
      "Iteration 2363, loss = 0.39966460\n",
      "Iteration 2364, loss = 0.39956298\n",
      "Iteration 2365, loss = 0.39946139\n",
      "Iteration 2366, loss = 0.39935984\n",
      "Iteration 2367, loss = 0.39925831\n",
      "Iteration 2368, loss = 0.39915683\n",
      "Iteration 2369, loss = 0.39905538\n",
      "Iteration 2370, loss = 0.39895396\n",
      "Iteration 2371, loss = 0.39885257\n",
      "Iteration 2372, loss = 0.39875122\n",
      "Iteration 2373, loss = 0.39864990\n",
      "Iteration 2374, loss = 0.39854862\n",
      "Iteration 2375, loss = 0.39844737\n",
      "Iteration 2376, loss = 0.39834615\n",
      "Iteration 2377, loss = 0.39824497\n",
      "Iteration 2378, loss = 0.39814382\n",
      "Iteration 2379, loss = 0.39804270\n",
      "Iteration 2380, loss = 0.39794162\n",
      "Iteration 2381, loss = 0.39784057\n",
      "Iteration 2382, loss = 0.39773955\n",
      "Iteration 2383, loss = 0.39763857\n",
      "Iteration 2384, loss = 0.39753762\n",
      "Iteration 2385, loss = 0.39743671\n",
      "Iteration 2386, loss = 0.39733583\n",
      "Iteration 2387, loss = 0.39723498\n",
      "Iteration 2388, loss = 0.39713416\n",
      "Iteration 2389, loss = 0.39703338\n",
      "Iteration 2390, loss = 0.39693264\n",
      "Iteration 2391, loss = 0.39683192\n",
      "Iteration 2392, loss = 0.39673124\n",
      "Iteration 2393, loss = 0.39663059\n",
      "Iteration 2394, loss = 0.39652998\n",
      "Iteration 2395, loss = 0.39642940\n",
      "Iteration 2396, loss = 0.39632885\n",
      "Iteration 2397, loss = 0.39622834\n",
      "Iteration 2398, loss = 0.39612786\n",
      "Iteration 2399, loss = 0.39602741\n",
      "Iteration 2400, loss = 0.39592699\n",
      "Iteration 2401, loss = 0.39582661\n",
      "Iteration 2402, loss = 0.39572626\n",
      "Iteration 2403, loss = 0.39562595\n",
      "Iteration 2404, loss = 0.39552567\n",
      "Iteration 2405, loss = 0.39542542\n",
      "Iteration 2406, loss = 0.39532521\n",
      "Iteration 2407, loss = 0.39522502\n",
      "Iteration 2408, loss = 0.39512488\n",
      "Iteration 2409, loss = 0.39502476\n",
      "Iteration 2410, loss = 0.39492468\n",
      "Iteration 2411, loss = 0.39482463\n",
      "Iteration 2412, loss = 0.39472461\n",
      "Iteration 2413, loss = 0.39462463\n",
      "Iteration 2414, loss = 0.39452468\n",
      "Iteration 2415, loss = 0.39442477\n",
      "Iteration 2416, loss = 0.39432488\n",
      "Iteration 2417, loss = 0.39422503\n",
      "Iteration 2418, loss = 0.39412521\n",
      "Iteration 2419, loss = 0.39402543\n",
      "Iteration 2420, loss = 0.39392568\n",
      "Iteration 2421, loss = 0.39382596\n",
      "Iteration 2422, loss = 0.39372627\n",
      "Iteration 2423, loss = 0.39362662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(), max_iter=3000, random_state=0,\n",
       "              verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(), max_iter=3000, random_state=0,\n",
       "              verbose=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(), max_iter=3000, random_state=0,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f9261d-845b-4fbf-9896-b2b327b305dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  [array([[1.02893169],\n",
      "       [0.99892496]])]\n"
     ]
    }
   ],
   "source": [
    "print('Weights: ', net.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a16eb25-f09a-4bab-a6eb-f03abd3625a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biases:  [array([-1.59323176])]\n"
     ]
    }
   ],
   "source": [
    "print('Biases: ', net.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1623d274-0520-4bc9-b2b0-1b20fdb54082",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = net.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75442e0-9bb1-4d4e-94c7-0cac3d2039b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcc2d80-2c8f-4305-be16-c36f6158dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(T,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6907be9b-3528-4cdc-96bd-2e088894f526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0],\n",
       "       [0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aefe705-0154-4704-9b0b-2674058561fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = net.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a978c2b-cbb9-4e9e-8d06-ea0b8bfa40af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8308302183712405,\n",
       " 0.8302257035704798,\n",
       " 0.8296221543729515,\n",
       " 0.8290195881460934,\n",
       " 0.8284180220020482,\n",
       " 0.8278174728400862,\n",
       " 0.8272179573276496,\n",
       " 0.8266194918718562,\n",
       " 0.8260220925895205,\n",
       " 0.8254257752778745,\n",
       " 0.8248305553868257,\n",
       " 0.8242364479931741,\n",
       " 0.8236434677770279,\n",
       " 0.8230516290005814,\n",
       " 0.8224609454893533,\n",
       " 0.8218714306159463,\n",
       " 0.8212830972863483,\n",
       " 0.8206959579287697,\n",
       " 0.8201100244849753,\n",
       " 0.8195253084040522,\n",
       " 0.8189418206385294,\n",
       " 0.8183595716427442,\n",
       " 0.8177785713733366,\n",
       " 0.8171988292917399,\n",
       " 0.8166203543685211,\n",
       " 0.816043155089424,\n",
       " 0.815467239462956,\n",
       " 0.8148926150293607,\n",
       " 0.8143192888708202,\n",
       " 0.8137472676227242,\n",
       " 0.8131765574858584,\n",
       " 0.8126071642393594,\n",
       " 0.8120390932542966,\n",
       " 0.811472349507747,\n",
       " 0.8109069375972313,\n",
       " 0.8103428617554018,\n",
       " 0.8097801258648682,\n",
       " 0.8092187334730625,\n",
       " 0.8086586878070593,\n",
       " 0.8080999917882651,\n",
       " 0.8075426480469091,\n",
       " 0.8069866589362763,\n",
       " 0.8064320265466239,\n",
       " 0.8058787527187438,\n",
       " 0.8053268390571249,\n",
       " 0.8047762869426938,\n",
       " 0.8042270975451037,\n",
       " 0.8036792718345568,\n",
       " 0.8031328105931449,\n",
       " 0.8025877144257045,\n",
       " 0.8020439837701799,\n",
       " 0.8015016189074943,\n",
       " 0.800960619970935,\n",
       " 0.8004209869550585,\n",
       " 0.799882719724122,\n",
       " 0.7993458180200559,\n",
       " 0.7988102814699873,\n",
       " 0.7982761095933304,\n",
       " 0.7977433018084559,\n",
       " 0.7972118574389607,\n",
       " 0.796681775719548,\n",
       " 0.7961530558015392,\n",
       " 0.7956256967580321,\n",
       " 0.7950996975887251,\n",
       " 0.7945750572244208,\n",
       " 0.7940517745312289,\n",
       " 0.7935298483144829,\n",
       " 0.7930092773223866,\n",
       " 0.7924900602494086,\n",
       " 0.7919721957394359,\n",
       " 0.7914556823887035,\n",
       " 0.7909405187485145,\n",
       " 0.7904267033277618,\n",
       " 0.7899142345952659,\n",
       " 0.7894031109819407,\n",
       " 0.7888933308827982,\n",
       " 0.7883848926588035,\n",
       " 0.7878777946385906,\n",
       " 0.7873720351200473,\n",
       " 0.7868676123717815,\n",
       " 0.7863645246344744,\n",
       " 0.7858627701221298,\n",
       " 0.7853623470232273,\n",
       " 0.7848632535017872,\n",
       " 0.7843654876983525,\n",
       " 0.7838690477308946,\n",
       " 0.7833739316956492,\n",
       " 0.7828801376678864,\n",
       " 0.7823876637026226,\n",
       " 0.7818965078352738,\n",
       " 0.7814066680822633,\n",
       " 0.7809181424415776,\n",
       " 0.780430928893282,\n",
       " 0.7799450253999951,\n",
       " 0.7794604299073289,\n",
       " 0.7789771403442934,\n",
       " 0.7784951546236717,\n",
       " 0.7780144706423665,\n",
       " 0.7775350862817219,\n",
       " 0.7770569994078191,\n",
       " 0.7765802078717539,\n",
       " 0.776104709509891,\n",
       " 0.7756305021441028,\n",
       " 0.7751575835819913,\n",
       " 0.7746859516170924,\n",
       " 0.7742156040290703,\n",
       " 0.773746538583896,\n",
       " 0.7732787530340165,\n",
       " 0.7728122451185128,\n",
       " 0.7723470125632472,\n",
       " 0.7718830530810059,\n",
       " 0.7714203643716272,\n",
       " 0.77095894412213,\n",
       " 0.7704987900068299,\n",
       " 0.770039899687453,\n",
       " 0.769582270813243,\n",
       " 0.7691259010210633,\n",
       " 0.7686707879354954,\n",
       " 0.7682169291689337,\n",
       " 0.7677643223216758,\n",
       " 0.767312964982011,\n",
       " 0.7668628547263036,\n",
       " 0.7664139891190765,\n",
       " 0.7659663657130912,\n",
       " 0.7655199820494261,\n",
       " 0.7650748356575513,\n",
       " 0.7646309240554056,\n",
       " 0.7641882447494677,\n",
       " 0.7637467952348298,\n",
       " 0.7633065729952684,\n",
       " 0.7628675755033139,\n",
       " 0.7624298002203203,\n",
       " 0.7619932445965344,\n",
       " 0.7615579060711629,\n",
       " 0.7611237820724409,\n",
       " 0.760690870017699,\n",
       " 0.7602591673134302,\n",
       " 0.7598286713553561,\n",
       " 0.7593993795284952,\n",
       " 0.758971289207227,\n",
       " 0.7585443977553604,\n",
       " 0.7581187025261996,\n",
       " 0.7576942008626102,\n",
       " 0.7572708900970861,\n",
       " 0.7568487675518157,\n",
       " 0.7564278305387501,\n",
       " 0.7560080763596686,\n",
       " 0.7555895023062462,\n",
       " 0.7551721056601224,\n",
       " 0.7547558836929679,\n",
       " 0.7543408336665514,\n",
       " 0.7539269528328106,\n",
       " 0.7535142384339192,\n",
       " 0.7531026877023552,\n",
       " 0.7526922978609715,\n",
       " 0.7522830661230651,\n",
       " 0.7518749896924463,\n",
       " 0.7514680657635102,\n",
       " 0.7510622915213062,\n",
       " 0.7506576641416101,\n",
       " 0.7502541807909942,\n",
       " 0.7498518386269004,\n",
       " 0.7494506347977123,\n",
       " 0.7490505664428259,\n",
       " 0.7486516306927248,\n",
       " 0.7482538246690527,\n",
       " 0.7478571454846872,\n",
       " 0.7474615902438131,\n",
       " 0.7470671560419979,\n",
       " 0.7466738399662671,\n",
       " 0.7462816390951779,\n",
       " 0.7458905504988965,\n",
       " 0.745500571239273,\n",
       " 0.7451116983699194,\n",
       " 0.744723928936284,\n",
       " 0.7443372599757315,\n",
       " 0.7439516885176183,\n",
       " 0.7435672115833724,\n",
       " 0.7431838261865702,\n",
       " 0.7428015293330168,\n",
       " 0.7424203180208238,\n",
       " 0.7420401892404896,\n",
       " 0.7416611399749796,\n",
       " 0.741283167199806,\n",
       " 0.7409062678831083,\n",
       " 0.7405304389857346,\n",
       " 0.7401556774613235,\n",
       " 0.7397819802563849,\n",
       " 0.7394093443103829,\n",
       " 0.7390377665558178,\n",
       " 0.7386672439183088,\n",
       " 0.7382977733166782,\n",
       " 0.737929351663033,\n",
       " 0.7375619758628508,\n",
       " 0.7371956428150626,\n",
       " 0.736830349412138,\n",
       " 0.7364660925401696,\n",
       " 0.736102869078958,\n",
       " 0.7357406759020984,\n",
       " 0.7353795098770649,\n",
       " 0.7350193678652976,\n",
       " 0.7346602467222891,\n",
       " 0.7343021432976702,\n",
       " 0.7339450544352982,\n",
       " 0.7335889769733432,\n",
       " 0.7332339077443767,\n",
       " 0.7328798435754583,\n",
       " 0.7325267812882249,\n",
       " 0.7321747176989788,\n",
       " 0.7318236496187769,\n",
       " 0.7314735738535183,\n",
       " 0.7311244872040356,\n",
       " 0.7307763864661829,\n",
       " 0.730429268430926,\n",
       " 0.7300831298844336,\n",
       " 0.7297379676081656,\n",
       " 0.7293937783789652,\n",
       " 0.7290505589691493,\n",
       " 0.7287083061465994,\n",
       " 0.7283670166748528,\n",
       " 0.7280266873131945,\n",
       " 0.7276873148167491,\n",
       " 0.727348895936571,\n",
       " 0.7270114274197397,\n",
       " 0.7266749060094492,\n",
       " 0.7263393284451022,\n",
       " 0.7260046914624023,\n",
       " 0.7256709917934475,\n",
       " 0.7253382261668224,\n",
       " 0.7250063913076927,\n",
       " 0.7246754839378982,\n",
       " 0.724345500776046,\n",
       " 0.7240164385376048,\n",
       " 0.7236882939350001,\n",
       " 0.7233610636777063,\n",
       " 0.7230347444723423,\n",
       " 0.7227093330227656,\n",
       " 0.7223848260301675,\n",
       " 0.7220612201931675,\n",
       " 0.7217385122079083,\n",
       " 0.72141669876815,\n",
       " 0.7210957765653669,\n",
       " 0.7207757422888419,\n",
       " 0.7204565926257614,\n",
       " 0.7201383242613125,\n",
       " 0.7198209338787762,\n",
       " 0.7195044181596256,\n",
       " 0.7191887737836193,\n",
       " 0.7188739974288998,\n",
       " 0.7185600857720874,\n",
       " 0.718247035488377,\n",
       " 0.7179348432516347,\n",
       " 0.7176235057344933,\n",
       " 0.7173130196084488,\n",
       " 0.7170033815439565,\n",
       " 0.7166945882105273,\n",
       " 0.7163866362768252,\n",
       " 0.7160795224107614,\n",
       " 0.7157732432795929,\n",
       " 0.7154677955500182,\n",
       " 0.7151631758882728,\n",
       " 0.7148593809602277,\n",
       " 0.7145564074314841,\n",
       " 0.7142542519674705,\n",
       " 0.713952911233539,\n",
       " 0.7136523818950625,\n",
       " 0.7133526606175308,\n",
       " 0.7130537440666458,\n",
       " 0.7127556289084196,\n",
       " 0.7124583118092696,\n",
       " 0.7121617894361156,\n",
       " 0.711866058456476,\n",
       " 0.7115711155385631,\n",
       " 0.7112769573513801,\n",
       " 0.710983580564817,\n",
       " 0.7106909818497461,\n",
       " 0.7103991578781187,\n",
       " 0.7101081053230599,\n",
       " 0.7098178208589648,\n",
       " 0.7095283011615946,\n",
       " 0.7092395429081716,\n",
       " 0.7089515427774737,\n",
       " 0.7086642974499313,\n",
       " 0.7083778036077213,\n",
       " 0.7080920579348624,\n",
       " 0.7078070571173102,\n",
       " 0.7075227978430509,\n",
       " 0.7072392768021977,\n",
       " 0.7069564906870831,\n",
       " 0.7066744361923545,\n",
       " 0.7063931100150674,\n",
       " 0.7061125088547796,\n",
       " 0.7058326294136453,\n",
       " 0.705553468396508,\n",
       " 0.7052750225109934,\n",
       " 0.7049972884676035,\n",
       " 0.7047202629798083,\n",
       " 0.7044439427641396,\n",
       " 0.7041683245402821,\n",
       " 0.7038934050311667,\n",
       " 0.7036191809630618,\n",
       " 0.7033456490656643,\n",
       " 0.7030728060721931,\n",
       " 0.7028006487194774,\n",
       " 0.7025291737480501,\n",
       " 0.7022583779022367,\n",
       " 0.7019882579302461,\n",
       " 0.7017188105842612,\n",
       " 0.7014500326205277,\n",
       " 0.7011819207994443,\n",
       " 0.7009144718856507,\n",
       " 0.700647682648118,\n",
       " 0.7003815498602352,\n",
       " 0.7001160702998991,\n",
       " 0.6998512407496007,\n",
       " 0.6995870579965138,\n",
       " 0.6993235188325815,\n",
       " 0.6990606200546023,\n",
       " 0.6987983584643178,\n",
       " 0.6985367308684975,\n",
       " 0.6982757340790252,\n",
       " 0.698015364912984,\n",
       " 0.6977556201927402,\n",
       " 0.6974964967460298,\n",
       " 0.6972379914060401,\n",
       " 0.6969801010114952,\n",
       " 0.6967228224067383,\n",
       " 0.6964661524418148,\n",
       " 0.6962100879725538,\n",
       " 0.6959546258606515,\n",
       " 0.6956997629737512,\n",
       " 0.6954454961855251,\n",
       " 0.6951918223757549,\n",
       " 0.6949387384304112,\n",
       " 0.6946862412417333,\n",
       " 0.6944343277083093,\n",
       " 0.6941829947351535,\n",
       " 0.693932239233785,\n",
       " 0.693682058122305,\n",
       " 0.6934324483254753,\n",
       " 0.6931834067747932,\n",
       " 0.6929349304085689,\n",
       " 0.6926870161719996,\n",
       " 0.6924396610172469,\n",
       " 0.6921928619035088,\n",
       " 0.6919466157970962,\n",
       " 0.6917009196715037,\n",
       " 0.6914557705074854,\n",
       " 0.6912111652931243,\n",
       " 0.6909671010239063,\n",
       " 0.6907235747027912,\n",
       " 0.6904805833402816,\n",
       " 0.6902381239544949,\n",
       " 0.6899961935712323,\n",
       " 0.6897547892240474,\n",
       " 0.6895139079543138,\n",
       " 0.6892735468112948,\n",
       " 0.6890337028522084,\n",
       " 0.688794373142295,\n",
       " 0.688555554754883,\n",
       " 0.6883172447714541,\n",
       " 0.688079440281707,\n",
       " 0.6878421383836234,\n",
       " 0.6876053361835295,\n",
       " 0.687369030796159,\n",
       " 0.6871332193447163,\n",
       " 0.6868978989609363,\n",
       " 0.686663066785147,\n",
       " 0.6864287199663275,\n",
       " 0.686194855662169,\n",
       " 0.6859614710391333,\n",
       " 0.6857285632725103,\n",
       " 0.6854961295464758,\n",
       " 0.6852641670541483,\n",
       " 0.6850326729976449,\n",
       " 0.6848016445881366,\n",
       " 0.6845710790459034,\n",
       " 0.6843409736003876,\n",
       " 0.684111325490247,\n",
       " 0.683882131963409,\n",
       " 0.6836533902771198,\n",
       " 0.6834250976979983,\n",
       " 0.6831972515020847,\n",
       " 0.6829698489748904,\n",
       " 0.6827428874114482,\n",
       " 0.6825163641163589,\n",
       " 0.6822902764038397,\n",
       " 0.6820646215977714,\n",
       " 0.6818393970317437,\n",
       " 0.6816146000491007,\n",
       " 0.6813902280029857,\n",
       " 0.681166278256385,\n",
       " 0.6809427481821706,\n",
       " 0.6807196351631438,\n",
       " 0.6804969365920749,\n",
       " 0.6802746498717456,\n",
       " 0.6800527724149883,\n",
       " 0.6798313016447266,\n",
       " 0.6796102349940117,\n",
       " 0.6793895699060627,\n",
       " 0.679169303834302,\n",
       " 0.6789494342423925,\n",
       " 0.6787299586042725,\n",
       " 0.6785108744041909,\n",
       " 0.6782921791367418,\n",
       " 0.6780738703068961,\n",
       " 0.6778559454300357,\n",
       " 0.6776384020319844,\n",
       " 0.6774212376490392,\n",
       " 0.6772044498279995,\n",
       " 0.6769880361261984,\n",
       " 0.6767719941115298,\n",
       " 0.6765563213624762,\n",
       " 0.6763410154681373,\n",
       " 0.676126074028255,\n",
       " 0.675911494653239,\n",
       " 0.6756972749641922,\n",
       " 0.6754834125929348,\n",
       " 0.6752699051820267,\n",
       " 0.6750567503847905,\n",
       " 0.6748439458653341,\n",
       " 0.6746314892985696,\n",
       " 0.674419378370235,\n",
       " 0.6742076107769139,\n",
       " 0.6739961842260525,\n",
       " 0.673785096435979,\n",
       " 0.6735743451359201,\n",
       " 0.6733639280660171,\n",
       " 0.6731538429773424,\n",
       " 0.6729440876319133,\n",
       " 0.6727346598027065,\n",
       " 0.6725255572736712,\n",
       " 0.6723167778397426,\n",
       " 0.6721083193068521,\n",
       " 0.6719001794919395,\n",
       " 0.6716923562229633,\n",
       " 0.6714848473389092,\n",
       " 0.6712776506898005,\n",
       " 0.6710707641367049,\n",
       " 0.6708641855517419,\n",
       " 0.6706579128180902,\n",
       " 0.6704519438299933,\n",
       " 0.6702462764927634,\n",
       " 0.6700409087227878,\n",
       " 0.6698358384475309,\n",
       " 0.669631063605538,\n",
       " 0.6694265821464371,\n",
       " 0.6692223920309408,\n",
       " 0.6690184912308471,\n",
       " 0.6688148777290384,\n",
       " 0.6686115495194827,\n",
       " 0.6684085046072308,\n",
       " 0.6682057410084143,\n",
       " 0.6680032567502449,\n",
       " 0.6678010498710071,\n",
       " 0.6675991184200581,\n",
       " 0.6673974604578211,\n",
       " 0.6671960740557794,\n",
       " 0.6669949572964713,\n",
       " 0.6667941082734827,\n",
       " 0.6665935250914398,\n",
       " 0.6663932058660013,\n",
       " 0.6661931487238486,\n",
       " 0.6659933518026772,\n",
       " 0.6657938132511867,\n",
       " 0.6655945312290693,\n",
       " 0.6653955039069988,\n",
       " 0.6651967294666185,\n",
       " 0.6649982061005292,\n",
       " 0.6647999320122745,\n",
       " 0.6646019054163289,\n",
       " 0.6644041245380805,\n",
       " 0.6642065876138191,\n",
       " 0.6640092928907181,\n",
       " 0.6638122386268188,\n",
       " 0.6636154230910143,\n",
       " 0.6634188445630306,\n",
       " 0.6632225013334099,\n",
       " 0.6630263917034905,\n",
       " 0.6628305139853893,\n",
       " 0.6626348665019803,\n",
       " 0.6624394475868758,\n",
       " 0.6622442555844034,\n",
       " 0.6620492888495869,\n",
       " 0.6618545457481233,\n",
       " 0.661660024656359,\n",
       " 0.6614657239612695,\n",
       " 0.6612716420604328,\n",
       " 0.6610777773620078,\n",
       " 0.6608841282847079,\n",
       " 0.6606906932577766,\n",
       " 0.6604974707209624,\n",
       " 0.6603044591244915,\n",
       " 0.6601116569290423,\n",
       " 0.6599190626057172,\n",
       " 0.6597266746360164,\n",
       " 0.6595344915118083,\n",
       " 0.6593425117353028,\n",
       " 0.6591507338190206,\n",
       " 0.6589591562857645,\n",
       " 0.6587677776685904,\n",
       " 0.6585765965107757,\n",
       " 0.658385611365789,\n",
       " 0.6581948207972597,\n",
       " 0.6580042233789453,\n",
       " 0.6578138176947,\n",
       " 0.657623602338443,\n",
       " 0.6574335759141243,\n",
       " 0.6572437370356927,\n",
       " 0.6570540843270621,\n",
       " 0.6568646164220765,\n",
       " 0.6566753319644776,\n",
       " 0.6564862296078685,\n",
       " 0.6562973080156794,\n",
       " 0.6561085658611322,\n",
       " 0.655920001827205,\n",
       " 0.6557316146065951,\n",
       " 0.6555434029016841,\n",
       " 0.6553553654244999,\n",
       " 0.6551675008966802,\n",
       " 0.6549798080494356,\n",
       " 0.6547922856235107,\n",
       " 0.6546049323691475,\n",
       " 0.6544177470460469,\n",
       " 0.6542307284233292,\n",
       " 0.6540438752794965,\n",
       " 0.6538571864023928,\n",
       " 0.653670660589165,\n",
       " 0.6534842966462238,\n",
       " 0.6532980933892024,\n",
       " 0.6531120496429184,\n",
       " 0.6529261642413312,\n",
       " 0.6527404360275036,\n",
       " 0.6525548638535597,\n",
       " 0.6523694465806437,\n",
       " 0.6521841830788793,\n",
       " 0.6519990722273284,\n",
       " 0.651814112913949,\n",
       " 0.6516293040355529,\n",
       " 0.6514446444977646,\n",
       " 0.6512601332149787,\n",
       " 0.6510757691103171,\n",
       " 0.6508915511155868,\n",
       " 0.6507074781712378,\n",
       " 0.6505235492263182,\n",
       " 0.6503397632384331,\n",
       " 0.6501561191737011,\n",
       " 0.6499726160067097,\n",
       " 0.649789252720473,\n",
       " 0.6496060283063879,\n",
       " 0.6494229417641894,\n",
       " 0.6492399921019084,\n",
       " 0.6490571783358262,\n",
       " 0.6488744994904314,\n",
       " 0.6486919545983746,\n",
       " 0.6485095427004255,\n",
       " 0.6483272628454274,\n",
       " 0.6481451140902533,\n",
       " 0.6479630954997608,\n",
       " 0.6477812061467487,\n",
       " 0.6475994451119106,\n",
       " 0.6474178114837912,\n",
       " 0.6472363043587415,\n",
       " 0.6470549228408737,\n",
       " 0.6468736660420159,\n",
       " 0.646692533081668,\n",
       " 0.6465115230869557,\n",
       " 0.6463306351925863,\n",
       " 0.6461498685408029,\n",
       " 0.6459692222813403,\n",
       " 0.645788695571379,\n",
       " 0.6456082875754998,\n",
       " 0.6454279974656402,\n",
       " 0.6452478244210478,\n",
       " 0.6450677676282351,\n",
       " 0.6448878262809365,\n",
       " 0.6447079995800598,\n",
       " 0.644528286733644,\n",
       " 0.6443486869568128,\n",
       " 0.6441691994717297,\n",
       " 0.6439898235075532,\n",
       " 0.6438105583003917,\n",
       " 0.6436314030932586,\n",
       " 0.643452357136027,\n",
       " 0.6432734196853852,\n",
       " 0.643094590004792,\n",
       " 0.6429158673644311,\n",
       " 0.6427372510411674,\n",
       " 0.6425587403185016,\n",
       " 0.6423803344865262,\n",
       " 0.6422020328418798,\n",
       " 0.6420238346877042,\n",
       " 0.6418457393335988,\n",
       " 0.6416677460955769,\n",
       " 0.6414898542960209,\n",
       " 0.641312063263639,\n",
       " 0.6411343723334204,\n",
       " 0.6409567808465916,\n",
       " 0.6407792881505724,\n",
       " 0.6406018935989323,\n",
       " 0.6404245965513465,\n",
       " 0.6402473963735535,\n",
       " 0.6400702924373096,\n",
       " 0.6398932841203473,\n",
       " 0.6397163708063311,\n",
       " 0.6395395518848151,\n",
       " 0.6393628267511994,\n",
       " 0.6391861948066875,\n",
       " 0.6390096554582436,\n",
       " 0.6388332081185497,\n",
       " 0.6386568522059634,\n",
       " 0.6384805871444756,\n",
       " 0.638304412363668,\n",
       " 0.6381283272986712,\n",
       " 0.6379523313901226,\n",
       " 0.637776424084125,\n",
       " 0.6376006048322046,\n",
       " 0.6374248730912696,\n",
       " 0.6372492283235687,\n",
       " 0.6370736699966499,\n",
       " 0.6368981975833206,\n",
       " 0.6367228105616046,\n",
       " 0.6365475084147029,\n",
       " 0.6363722906309531,\n",
       " 0.6361971567037883,\n",
       " 0.6360221061316974,\n",
       " 0.6358471384181849,\n",
       " 0.6356722530717309,\n",
       " 0.6354974496057514,\n",
       " 0.6353227275385591,\n",
       " 0.6351480863933234,\n",
       " 0.6349735256980317,\n",
       " 0.6347990449854499,\n",
       " 0.6346246437930847,\n",
       " 0.634450321663143,\n",
       " 0.6342760781424954,\n",
       " 0.6341019127826361,\n",
       " 0.6339278251396468,\n",
       " 0.6337538147741567,\n",
       " 0.6335798812513063,\n",
       " 0.633406024140709,\n",
       " 0.6332322430164137,\n",
       " 0.6330585374568687,\n",
       " 0.6328849070448828,\n",
       " 0.6327113513675908,\n",
       " 0.6325378700164147,\n",
       " 0.6323644625870292,\n",
       " 0.6321911286793238,\n",
       " 0.6320178678973681,\n",
       " 0.631844679849375,\n",
       " 0.6316715641476662,\n",
       " 0.6314985204086355,\n",
       " 0.6313255482527147,\n",
       " 0.6311526473043373,\n",
       " 0.630979817191905,\n",
       " 0.6308070575477527,\n",
       " 0.6306343680081131,\n",
       " 0.6304617482130841,\n",
       " 0.6302891978065933,\n",
       " 0.6301167164363652,\n",
       " 0.6299443037538871,\n",
       " 0.6297719594143758,\n",
       " 0.6295996830767443,\n",
       " 0.6294274744035689,\n",
       " 0.6292553330610566,\n",
       " 0.6290832587190119,\n",
       " 0.6289112510508056,\n",
       " 0.6287393097333407,\n",
       " 0.6285674344470223,\n",
       " 0.628395624875725,\n",
       " 0.6282238807067613,\n",
       " 0.6280522016308501,\n",
       " 0.6278805873420866,\n",
       " 0.6277090375379097,\n",
       " 0.6275375519190726,\n",
       " 0.6273661301896115,\n",
       " 0.6271947720568156,\n",
       " 0.6270234772311971,\n",
       " 0.6268522454264609,\n",
       " 0.626681076359475,\n",
       " 0.6265099697502414,\n",
       " 0.6263389253218661,\n",
       " 0.6261679428005303,\n",
       " 0.625997021915462,\n",
       " 0.6258261623989068,\n",
       " 0.6256553639860991,\n",
       " 0.625484626415234,\n",
       " 0.6253139494274403,\n",
       " 0.6251433327667512,\n",
       " 0.6249727761800768,\n",
       " 0.6248022794171777,\n",
       " 0.6246318422306364,\n",
       " 0.6244614643758313,\n",
       " 0.6242911456109089,\n",
       " 0.6241208856967579,\n",
       " 0.6239506843969825,\n",
       " 0.6237805414778753,\n",
       " 0.6236104567083929,\n",
       " 0.6234404298601287,\n",
       " 0.6232704607072874,\n",
       " 0.6231005490266601,\n",
       " 0.6229306945975984,\n",
       " 0.6227608972019897,\n",
       " 0.6225911566242315,\n",
       " 0.6224214726512087,\n",
       " 0.6222518450722665,\n",
       " 0.6220822736791878,\n",
       " 0.6219127582661684,\n",
       " 0.6217432986297939,\n",
       " 0.6215738945690147,\n",
       " 0.6214045458851232,\n",
       " 0.6212352523817303,\n",
       " 0.6210660138647426,\n",
       " 0.620896830142339,\n",
       " 0.6207277010249475,\n",
       " 0.6205586263252236,\n",
       " 0.620389605858027,\n",
       " 0.6202206394404001,\n",
       " 0.6200517268915453,\n",
       " 0.6198828680328031,\n",
       " 0.6197140626876314,\n",
       " 0.6195453106815825,\n",
       " 0.6193766118422835,\n",
       " 0.6192079659994133,\n",
       " 0.6190393729846829,\n",
       " 0.6188708326318145,\n",
       " 0.6187023447765202,\n",
       " 0.6185339092564824,\n",
       " 0.6183655259113331,\n",
       " 0.6181971945826333,\n",
       " 0.6180289151138545,\n",
       " 0.6178606873503574,\n",
       " 0.6176925111393734,\n",
       " 0.6175243863299849,\n",
       " 0.6173563127731052,\n",
       " 0.6171882903214616,\n",
       " 0.6170203188295743,\n",
       " 0.6168523981537383,\n",
       " 0.6166845281520058,\n",
       " 0.6165167086841662,\n",
       " 0.6163489396117297,\n",
       " 0.6161812207979072,\n",
       " 0.6160135521075947,\n",
       " 0.6158459334073534,\n",
       " 0.6156783645653928,\n",
       " 0.6155108454515548,\n",
       " 0.6153433759372938,\n",
       " 0.6151759558956614,\n",
       " 0.6150085852012893,\n",
       " 0.6148412637303717,\n",
       " 0.6146739913606495,\n",
       " 0.6145067679713929,\n",
       " 0.6143395934433862,\n",
       " 0.6141724676589108,\n",
       " 0.6140053905017291,\n",
       " 0.6138383618570692,\n",
       " 0.6136713816116087,\n",
       " 0.6135044496534593,\n",
       " 0.6133375658721512,\n",
       " 0.613170730158618,\n",
       " 0.6130039424051812,\n",
       " 0.6128372025055359,\n",
       " 0.6126705103547346,\n",
       " 0.6125038658491743,\n",
       " 0.6123372688865801,\n",
       " 0.6121707193659923,\n",
       " 0.6120042171877506,\n",
       " 0.611837762253481,\n",
       " 0.6116713544660813,\n",
       " 0.6115049937297079,\n",
       " 0.6113386799497605,\n",
       " 0.6111724130328702,\n",
       " 0.6110061928868847,\n",
       " 0.6108400194208556,\n",
       " 0.6106738925450249,\n",
       " 0.6105078121708121,\n",
       " 0.6103417782108009,\n",
       " 0.6101757905787266,\n",
       " 0.610009849189463,\n",
       " 0.6098439539590107,\n",
       " 0.6096781048044831,\n",
       " 0.6095123016440954,\n",
       " 0.6093465443971523,\n",
       " 0.6091808329840342,\n",
       " 0.6090151673261875,\n",
       " 0.6088495473461115,\n",
       " 0.6086839729673464,\n",
       " 0.6085184441144624,\n",
       " 0.6083529607130476,\n",
       " 0.608187522689697,\n",
       " 0.6080221299720009,\n",
       " 0.6078567824885338,\n",
       " 0.6076914801688436,\n",
       " 0.60752622294344,\n",
       " 0.6073610107437847,\n",
       " 0.6071958435022791,\n",
       " 0.6070307211522553,\n",
       " 0.6068656436279647,\n",
       " 0.6067006108645678,\n",
       " 0.6065356227981235,\n",
       " 0.6063706793655796,\n",
       " 0.606205780504762,\n",
       " 0.6060409261543657,\n",
       " 0.6058761162539437,\n",
       " 0.6057113507438976,\n",
       " 0.6055466295654689,\n",
       " 0.6053819526607276,\n",
       " 0.6052173199725644,\n",
       " 0.6050527314446805,\n",
       " 0.604888187021578,\n",
       " 0.6047236866485516,\n",
       " 0.6045592302716789,\n",
       " 0.6043948178378116,\n",
       " 0.6042304492945658,\n",
       " 0.604066124590315,\n",
       " 0.6039018436741794,\n",
       " 0.6037376064960188,\n",
       " 0.6035734130064224,\n",
       " 0.6034092631567025,\n",
       " 0.6032451568988838,\n",
       " 0.6030810941856976,\n",
       " 0.6029170749705707,\n",
       " 0.6027530992076201,\n",
       " 0.6025891668516434,\n",
       " 0.6024252778581114,\n",
       " 0.6022614321831596,\n",
       " 0.6020976297835814,\n",
       " 0.6019338706168199,\n",
       " 0.6017701546409597,\n",
       " 0.601606481814721,\n",
       " 0.6014428520974506,\n",
       " 0.6012792654491151,\n",
       " 0.6011157218302942,\n",
       " 0.6009522212021717,\n",
       " 0.6007887635265313,\n",
       " 0.6006253487657468,\n",
       " 0.6004619768827759,\n",
       " 0.6002986478411546,\n",
       " 0.6001353616049888,\n",
       " 0.5999721181389481,\n",
       " 0.5998089174082595,\n",
       " 0.5996457593786999,\n",
       " 0.5994826440165905,\n",
       " 0.5993195712887901,\n",
       " 0.5991565411626882,\n",
       " 0.5989935536061992,\n",
       " 0.5988306085877562,\n",
       " 0.5986677060763042,\n",
       " 0.5985048460412944,\n",
       " 0.5983420284526789,\n",
       " 0.5981792532809032,\n",
       " 0.5980165204969011,\n",
       " 0.5978538300720891,\n",
       " 0.5976911819783601,\n",
       " 0.5975285761880779,\n",
       " 0.5973660126740716,\n",
       " 0.59720349140963,\n",
       " 0.5970410123684956,\n",
       " 0.5968785755248598,\n",
       " 0.5967161808533573,\n",
       " 0.5965538283290605,\n",
       " 0.5963915179274739,\n",
       " 0.5962292496245299,\n",
       " 0.596067023396583,\n",
       " 0.5959048392204042,\n",
       " 0.5957426970731768,\n",
       " 0.5955805969324909,\n",
       " 0.5954185387763387,\n",
       " 0.5952565225831093,\n",
       " 0.595094548331584,\n",
       " 0.5949326160009314,\n",
       " 0.5947707255707031,\n",
       " 0.5946088770208288,\n",
       " 0.5944470703316111,\n",
       " 0.5942853054837215,\n",
       " 0.5941235824581963,\n",
       " 0.5939619012364308,\n",
       " 0.5938002618001763,\n",
       " 0.5936386641315345,\n",
       " 0.5934771082129544,\n",
       " 0.5933155940272268,\n",
       " 0.593154121557481,\n",
       " 0.5929926907871804,\n",
       " 0.5928313017001173,\n",
       " 0.5926699542804109,\n",
       " 0.5925086485125014,\n",
       " 0.5923473843811469,\n",
       " 0.5921861618714188,\n",
       " 0.5920249809686992,\n",
       " 0.5918638416586756,\n",
       " 0.5917027439273373,\n",
       " 0.5915416877609725,\n",
       " 0.5913806731461642,\n",
       " 0.5912197000697855,\n",
       " 0.5910587685189975,\n",
       " 0.590897878481245,\n",
       " 0.5907370299442523,\n",
       " 0.5905762228960206,\n",
       " 0.5904154573248245,\n",
       " 0.5902547332192076,\n",
       " 0.5900940505679804,\n",
       " 0.5899334093602159,\n",
       " 0.5897728095852466,\n",
       " 0.5896122512326611,\n",
       " 0.5894517342923019,\n",
       " 0.5892912587542597,\n",
       " 0.5891308246088732,\n",
       " 0.5889704318467238,\n",
       " 0.5888100804586337,\n",
       " 0.5886497704356615,\n",
       " 0.5884895017691011,\n",
       " 0.5883292744504769,\n",
       " 0.5881690884715418,\n",
       " 0.5880089438242739,\n",
       " 0.5878488405008739,\n",
       " 0.5876887784937616,\n",
       " 0.5875287577955742,\n",
       " 0.5873687783991621,\n",
       " 0.5872088402975874,\n",
       " 0.5870489434841201,\n",
       " 0.5868890879522363,\n",
       " 0.5867292736956146,\n",
       " 0.5865695007081346,\n",
       " 0.5864097689838733,\n",
       " 0.586250078517103,\n",
       " 0.5860904293022884,\n",
       " 0.5859308213340844,\n",
       " 0.5857712546073334,\n",
       " 0.5856117291170634,\n",
       " 0.5854522448584842,\n",
       " 0.5852928018269867,\n",
       " 0.5851334000181392,\n",
       " 0.5849740394276859,\n",
       " 0.5848147200515438,\n",
       " 0.5846554418858011,\n",
       " 0.5844962049267146,\n",
       " 0.5843370091707073,\n",
       " 0.5841778546143666,\n",
       " 0.5840187412544415,\n",
       " 0.5838596690878415,\n",
       " 0.5837006381116329,\n",
       " 0.5835416483230381,\n",
       " 0.5833826997194326,\n",
       " 0.5832237922983435,\n",
       " 0.5830649260574473,\n",
       " 0.5829061009945671,\n",
       " 0.5827473171076724,\n",
       " 0.5825885743948749,\n",
       " 0.5824298728544282,\n",
       " 0.5822712124847255,\n",
       " 0.5821125932842971,\n",
       " 0.5819540152518089,\n",
       " 0.5817954783860613,\n",
       " 0.5816369826859853,\n",
       " 0.5814785281506434,\n",
       " 0.5813201147792251,\n",
       " 0.5811617425710477,\n",
       " 0.581003411525552,\n",
       " 0.5808451216423027,\n",
       " 0.5806868729209855,\n",
       " 0.5805286653614055,\n",
       " 0.5803704989634861,\n",
       " 0.5802123737272664,\n",
       " 0.5800542896529007,\n",
       " 0.5798962467406559,\n",
       " 0.5797382449909105,\n",
       " 0.5795802844041528,\n",
       " 0.5794223649809788,\n",
       " 0.579264486722092,\n",
       " 0.5791066496283005,\n",
       " 0.5789488537005165,\n",
       " 0.5787910989397538,\n",
       " 0.5786333853471274,\n",
       " 0.5784757129238509,\n",
       " 0.5783180816712363,\n",
       " 0.5781604915906919,\n",
       " 0.5780029426837205,\n",
       " 0.5778454349519186,\n",
       " 0.5776879683969751,\n",
       " 0.5775305430206694,\n",
       " 0.5773731588248708,\n",
       " 0.5772158158115359,\n",
       " 0.5770585139827092,\n",
       " 0.57690125334052,\n",
       " 0.5767440338871819,\n",
       " 0.5765868556249917,\n",
       " 0.5764297185563279,\n",
       " 0.5762726226836491,\n",
       " 0.5761155680094936,\n",
       " 0.5759585545364776,\n",
       " 0.575801582267294,\n",
       " 0.5756446512047116,\n",
       " 0.5754877613515735,\n",
       " 0.5753309127107962,\n",
       " 0.5751741052853683,\n",
       " 0.5750173390783494,\n",
       " 0.5748606140928697,\n",
       " 0.5747039303321272,\n",
       " 0.5745472877993887,\n",
       " 0.5743906864979867,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882e5748-97ce-4cac-bbc9-ef331a4b7f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2927ca648c0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba934159-a1ed-4c10-b833-86ec795cfc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myNN",
   "language": "python",
   "name": "mynn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
